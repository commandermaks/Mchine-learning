{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/commandermaks/Mchine-learning/blob/main/Naive_Bayes_A3_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJn07BVq4HHy"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this notebook, we will explore a machine learning classifier – the Naive Bayes classifier, which uses the Bayes’ theorem to classify test examples into one of the defined classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23EH8M7T4HH0"
      },
      "source": [
        "## Bayes' Theorem\n",
        "\n",
        "In Statistics and probability theory, Bayes’ theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is a simple mathematical formula used for calculating conditional probabilities.\n",
        "\n",
        "Conditional probability is a measure of the probability of an event occurring given that another event has (by assumption, presumption, assertion, or evidence) occurred.\n",
        "\n",
        "        P(A|B) = (P(B|A).P(A))/P(B)\n",
        "\n",
        "where  P(A|B) is the probability of A occuring given B has already occured, P(B|A) is the probability of B occuring given A has already occured, P(A) is the probability of A occuring and P(B) is the probability of B occuring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GJo_SkN4HH0"
      },
      "source": [
        "## Bayes' Theorem Example\n",
        "\n",
        "Let’s suppose we have a Deck of Cards, we wish to find out the “Probability of the Card we picked at random to be a King given that it is a Face Card“. So, according to Bayes Theorem, we can solve this problem. First, we need to find out the probability\n",
        "\n",
        "     1. P(King) which is 4/52 as there are 4 Kings in a Deck of Cards.\n",
        "     2. P(Face|King) is equal to 1 as all the Kings are face Cards.\n",
        "     3. P(Face) is equal to 12/52 as there are 3 Face Cards in a Suit of 13 cards and there are 4 Suits in total.\n",
        "     \n",
        "   Now, putting all the values in the Bayes’ Equation\n",
        "   \n",
        "          P(King|Face) = (P(Face|King).P(King)) / P(Face)\n",
        "                       = (1.(1/13))/(3/13)\n",
        "                       = 1/3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaYO2-aN4HH1"
      },
      "source": [
        "## Naive Bayes Classifier\n",
        "\n",
        "Naive Bayes classifiers are a family of probabilistic classifiers that are based on Bayes’ theorem. It is used to solve many different problem statements, and it is quite fast in training a model since Naive Bayes classifier completely works on probability, so the conversion happens quickly. These algorithms work by combining the probabilities that an instance belongs to a class based on the value of a set of features.\n",
        "\n",
        "Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive.\n",
        "\n",
        "The fundamental Naïve Bayes assumption is that each feature makes an independent and equal contribution to the outcome.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9i1st3g4HH2"
      },
      "source": [
        "## Types of Naive Bayes Classifier\n",
        "\n",
        "1. Multinomial Naive Bayes:\n",
        "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
        "\n",
        "2. Bernoulli Naive Bayes:\n",
        "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
        "\n",
        "3. Gaussian Naive Bayes:\n",
        "In Gaussian Naïve Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution (Normal distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfg58k1T4HH2"
      },
      "source": [
        "## How does Naive Bayes classifier work?\n",
        "\n",
        "Naive Bayes methods work by applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
        "\n",
        "The Bayes’ theorem states that for a given class variable y and dependent  vector $ x = x_{1} + x_{2} ... x_{n}.$\n",
        "\n",
        "The conditional probability of the class label $y$, given the observation $x$ is:\n",
        "\n",
        "  ![Conditional_prob.png](attachment:Conditional_prob.png)\n",
        "     \n",
        "We can simplify the above expression using the naive assumption that features of measurement are independent of each other, i.e."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDmH6hNg4HH3"
      },
      "source": [
        "![Second_equation.png](attachment:Second_equation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDY07vV34HH3"
      },
      "source": [
        "which lead to:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynec3Ivu4HH3"
      },
      "source": [
        "![Third_equation.png](attachment:Third_equation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwyeLK9T4HH3"
      },
      "source": [
        "Since, $ P(x_{1},..,x_{n}) $ is constant given the input, we use Maximum A Posteriori (MAP) estimation to estimate P(y) and $ P(x_{i} ∣ y) $, which results in:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLOHHtG04HH4"
      },
      "source": [
        "![MAP_equation.png](attachment:MAP_equation.png)\n",
        "\n",
        "This calculation can be performed for each of the class labels, and the label with the largest probability can be selected as the classification for the given instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6dBL7V84HH4"
      },
      "source": [
        "## Step By Step Implementation of Naive Bayes\n",
        "\n",
        "    1. Handle Data\n",
        "    2. Summarize Data\n",
        "    3. Make Predictions\n",
        "    4. Evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V40RDPup4HH4"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "This Iris dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\n",
        "Attribute Information:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "5. class: Iris Setosa, Iris Versicolour, Iris Virginica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBgQvBFI4HH5"
      },
      "source": [
        "## Step 1: Handle Data\n",
        "\n",
        "The first thing we need to do is load our data file. The data is in CSV format. We can open the file with the read_csv function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vAmU5JV24HH5"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O\n",
        "import os\n",
        "from collections import Counter\n",
        "import math\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zgK_ff9I4HH6",
        "outputId": "0b09f605-2554-4b1d-cca7-c9be360915de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bf2cc5383e4c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miris\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Iris.csv'"
          ]
        }
      ],
      "source": [
        "iris = pd.read_csv('/content/Iris.csv')\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSo674c4HH7"
      },
      "source": [
        "## Step 2: Summarize the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvgx8t6O4HH7"
      },
      "outputs": [],
      "source": [
        "iris.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8FvTjEW4HH7"
      },
      "outputs": [],
      "source": [
        "target_category = iris[\"Species\"].unique()\n",
        "target_category=list(map(str,target_category))\n",
        "print(target_category)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNh4D1xR4HH8"
      },
      "source": [
        "**Data Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYT5mPvL4HH8"
      },
      "outputs": [],
      "source": [
        "iris.groupby(\"Species\").Species.count().plot.bar(ylim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDymD-DU4HH8"
      },
      "outputs": [],
      "source": [
        "species = iris.Species\n",
        "data = iris.drop(columns=['Species','Id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8dQ2Xoj4HH8"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyLEJMrd4HH9"
      },
      "outputs": [],
      "source": [
        "iris['Category'] = iris['Species'].factorize()[0]\n",
        "category = iris['Category']\n",
        "iris.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVhlKNaA4HH9"
      },
      "source": [
        "**Split dataset into train/test**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPpcDGK34HH9"
      },
      "outputs": [],
      "source": [
        "#split dataset into test set(20%) and train set(80%) using stratify to split into equal size\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train,data_test,species_train,species_test = train_test_split(data,category, test_size = 0.2, stratify = category,random_state=1)\n",
        "print(np.bincount(species_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbU-UW_c4HH-"
      },
      "outputs": [],
      "source": [
        "newIris = pd.DataFrame(np.column_stack([data_train,species_train]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJIRMU3D4HH-"
      },
      "outputs": [],
      "source": [
        "#Sort and rearrange the data based on the species\n",
        "\n",
        "setosa = newIris[newIris[4] == 0]\n",
        "versicolor = newIris[newIris[4]==1]\n",
        "virginica  = newIris[newIris[4]==2]\n",
        "newIris = pd.concat([setosa,versicolor,virginica])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newIris.head()"
      ],
      "metadata": {
        "id": "rSpkIRfhGiWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq72e69K4HH-"
      },
      "outputs": [],
      "source": [
        "#Splits data based on species\n",
        "\n",
        "setosa_data=newIris[0:40]\n",
        "versicolor_data=newIris[40:80]\n",
        "virginica_data=newIris[80:120]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYEauD4X4HH-"
      },
      "source": [
        "**Find mean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yL47yfXs4HH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "setosa_mean = setosa_data.mean()\n",
        "versicolor_mean = versicolor_data.mean()\n",
        "virginica_mean= virginica_data.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0fCkmG84HH_"
      },
      "source": [
        "**Find standard deviation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xa4KtHK4HH_"
      },
      "outputs": [],
      "source": [
        "\n",
        "setosa_std = setosa_data.std()\n",
        "versicolor_std = versicolor_data.std()\n",
        "virginica_std = virginica_data.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3fFOIJP4HH_"
      },
      "source": [
        "## Step 3: Make Predictions\n",
        "\n",
        "We are now ready to make predictions using the summaries prepared from our training data. Making predictions involves calculating the probability that a given data instance belongs to each class, then selecting the class with the largest probability as the prediction. We need to perform the following tasks:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean\n",
        "\n"
      ],
      "metadata": {
        "id": "Eajr6PycMxXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSWZJpJA4HH_"
      },
      "source": [
        "**Finding Likelihood**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIeGUo864HIA"
      },
      "outputs": [],
      "source": [
        "\n",
        "x =[]\n",
        "likelihood = []\n",
        "\n",
        "for j in range(len(newIris)):\n",
        "    distribution = 1\n",
        "    if(j<40):\n",
        "        mean=setosa_mean\n",
        "        std = setosa_std\n",
        "    if(j>=40 and j<80):\n",
        "        mean=versicolor_mean\n",
        "        std = versicolor_std\n",
        "    if(j>=80 and j<120):\n",
        "        mean=virginica_mean\n",
        "        std = virginica_std\n",
        "\n",
        "    for i in range(4):\n",
        "        x = newIris.iloc[j]\n",
        "        a= ((x[i]- mean[i])**2)/(2*std[i]**2)\n",
        "        b= math.sqrt(2*math.pi*(std[i]**2))\n",
        "        y = math.exp(-a)/b\n",
        "        distribution= distribution*y\n",
        "    likelihood.append(distribution)\n",
        "    x=[]\n",
        "print(likelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avvU9FYL4HIA"
      },
      "source": [
        "**Find Priori probability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnNtEkjm4HIA"
      },
      "outputs": [],
      "source": [
        "\n",
        "setosa_priori = len(setosa_data)/len(newIris)\n",
        "versicolor_priori = len(versicolor_data)/len(newIris)\n",
        "virginica_priori = len(virginica_data)/len(newIris)\n",
        "\n",
        "print(setosa_priori)\n",
        "print(versicolor_priori)\n",
        "print(virginica_priori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1UqCI2W4HIB"
      },
      "outputs": [],
      "source": [
        "newTest = pd.DataFrame(np.column_stack([data_test,species_test]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RbFZqmB4HIB"
      },
      "source": [
        "**Rearrange the data into groups based on the species**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlxtXfL-4HIB"
      },
      "outputs": [],
      "source": [
        "setosa = newTest[newTest[4] == 0]\n",
        "versicolor = newTest[newTest[4]==1]\n",
        "virginica  = newTest[newTest[4]==2]\n",
        "newTest = pd.concat([setosa,versicolor,virginica])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwMIVgwo4HIB"
      },
      "source": [
        "**Find likelihood for test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caDg8-kx4HIB"
      },
      "outputs": [],
      "source": [
        "testLikelihood =[]\n",
        "x=[]\n",
        "testPosterior=[]\n",
        "posteriorSpecies =[]\n",
        "\n",
        "\n",
        "for j in range(len(newTest)):\n",
        "    for c in range(3):\n",
        "        if (c==0):\n",
        "            mean = setosa_mean\n",
        "            std = setosa_std\n",
        "            priori = setosa_priori\n",
        "        if (c == 1):\n",
        "            mean= versicolor_mean\n",
        "            std = versicolor_std\n",
        "            priori = versicolor_priori\n",
        "        if(c == 2):\n",
        "            mean= virginica_mean\n",
        "            std = virginica_std\n",
        "            priori = virginica_priori\n",
        "        distribution = 1\n",
        "        for i in range(4):\n",
        "            x = newTest.iloc[j]\n",
        "            a= ((x[i]- mean[i])**2)/(2*std[i]**2)\n",
        "            b= math.sqrt(2*math.pi*(std[i]**2))\n",
        "            y = math.exp(-a)/b\n",
        "            distribution= distribution*y\n",
        "        x=[]\n",
        "        testLikelihood.append(distribution)\n",
        "        posterior = testLikelihood[c]* priori    #Calculate posterior values\n",
        "        testPosterior.append(posterior)\n",
        "        maxPosterior = testPosterior.index(max(testPosterior))   #Finds the maximum value\n",
        "    posteriorSpecies.append(maxPosterior)\n",
        "\n",
        "\n",
        "    testLikelihood =[]\n",
        "    testPosterior=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKfbM8RV4HIC"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV9Vx4AS4HIC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(species_test, posteriorSpecies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps5Lrpci4HIC"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "\n",
        "The pima-indians-diabetes.csv dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. It is a binary classification dataset. Several constraints were placed on the selection of instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The objective is to predict if a patient has diabetes based on diagnostic measurements of eight simple features.\n",
        "\n",
        "Attributes:\n",
        "1. preg = Number of times pregnant\n",
        "2. plas = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. pres = Diastolic blood pressure (mm Hg)\n",
        "4. skin = Triceps skin fold thickness (mm)\n",
        "5. test = 2-Hour serum insulin (mu U/ml)\n",
        "6. mass = Body mass index (weight in kg/(height in m)^2)\n",
        "7. pedi = Diabetes pedigree function\n",
        "8. age = Age (years)\n",
        "9. class = Class variable (1:tested positive for diabetes, 0: tested negative for diabetes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jteiaz6v4HIC"
      },
      "source": [
        "### Q1. Write a program to implement the Naive Bayesian classifier for a data set stored as a pima-indians-diabetes.csv file  to predict class labels of test data. The predicted class labels for the test instances should be printed as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psK5i5fZ4HID"
      },
      "source": [
        "**a. Compute the accuracy of the classifier.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FkeCQyj4HID"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/pima-indians-diabetes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "leAto19MAdSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['class'].value_counts()"
      ],
      "metadata": {
        "id": "GKKJ3tQ3CC26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = df.drop('class', axis=1)\n",
        "data1.head()"
      ],
      "metadata": {
        "id": "RCAOwslGEz9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = df['class']\n",
        "classes.head()"
      ],
      "metadata": {
        "id": "YMHsB7IRCjYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "data_train,data_test,class_train,class_test = train_test_split(data1,classes, test_size = 0.2, stratify = classes,random_state=1)\n",
        "print(np.bincount(class_train))"
      ],
      "metadata": {
        "id": "w5AhiHanAgEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newData = pd.DataFrame(np.column_stack([data_train,class_train]))\n",
        "newData.head()"
      ],
      "metadata": {
        "id": "zFpgg_JjDzTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_0 = newData[newData[8] == 0]\n",
        "class_1 = newData[newData[8]==1]\n",
        "newData = pd.concat([class_0, class_1])"
      ],
      "metadata": {
        "id": "SmVdR-7gEGMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(class_0)"
      ],
      "metadata": {
        "id": "sY3OoE_FHSIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newData.head()"
      ],
      "metadata": {
        "id": "jnZobrkGGMJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newData.groupby(8).count()"
      ],
      "metadata": {
        "id": "qnLKZEDkGupN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_0=newIris[0:400]\n",
        "data_1=newIris[400:614]"
      ],
      "metadata": {
        "id": "4Cglpo9GHlGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_0 = data_0.mean()\n",
        "mean_1 = data_1.mean()"
      ],
      "metadata": {
        "id": "bQHLbMx0ICpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_0 = data_0.std()\n",
        "std_1 = data_1.std()"
      ],
      "metadata": {
        "id": "5SsU-ng0IJPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newData.head()"
      ],
      "metadata": {
        "id": "Gw95e1AVKiN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1 = []\n",
        "std1 = []\n",
        "for i in range(8):\n",
        "  mean1.append(newData[i].mean())\n",
        "  std1.append(newData[i].std())"
      ],
      "metadata": {
        "id": "XSFypQDMM5C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1"
      ],
      "metadata": {
        "id": "RUGFaO9PNfIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x =[]\n",
        "likelihood = []\n",
        "\n",
        "for j in range(len(newData)):\n",
        "    distribution = 1\n",
        "    if(j<400):\n",
        "        mean = mean_0\n",
        "        std = std_0\n",
        "    if(j>=400 and j<614):\n",
        "        mean=mean_1\n",
        "        std = std_1\n",
        "\n",
        "    for i in range(8):\n",
        "        x = newData.iloc[j]\n",
        "        a= ((x[i]- mean1[i])**2)/(2*std1[i]**2)\n",
        "        b= math.sqrt(2*math.pi*(std1[i]**2))\n",
        "        y = math.exp(-a)/b\n",
        "        distribution= distribution*y\n",
        "    likelihood.append(distribution)\n",
        "    x=[]\n",
        "print(likelihood)"
      ],
      "metadata": {
        "id": "2nyRp6NaIP_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newTest1 = pd.DataFrame(np.column_stack([data_test,class_test]))"
      ],
      "metadata": {
        "id": "b0Ahn5fGN9T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newTest1"
      ],
      "metadata": {
        "id": "ThaIdAqUOPuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_0 = newTest1[newTest1[8] == 0]\n",
        "test_1 = newTest1[newTest1[8]==1]\n",
        "newTest1 = pd.concat([test_0, test_1])"
      ],
      "metadata": {
        "id": "hKEUD7CpOlwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FTnDNLtPNos"
      },
      "outputs": [],
      "source": [
        "testLikelihood1 =[]\n",
        "x=[]\n",
        "testPosterior=[]\n",
        "posteriorSpecies =[]\n",
        "\n",
        "\n",
        "for j in range(len(newTest1)):\n",
        "    for c in range(2):\n",
        "        if (c==0):\n",
        "            mean = mean_0\n",
        "            std = std_0\n",
        "            priori = len(data_0)/len(newData)\n",
        "        if (c == 1):\n",
        "            mean= mean_1\n",
        "            std = std_1\n",
        "            priori = len(data_1)/len(newData)\n",
        "        distribution = 1\n",
        "        for i in range(8):\n",
        "            x = newTest1.iloc[j]\n",
        "            a= ((x[i]- mean1[i])**2)/(2*std1[i]**2)\n",
        "            b= math.sqrt(2*math.pi*(std1[i]**2))\n",
        "            y = math.exp(-a)/b\n",
        "            distribution= distribution*y\n",
        "        x=[]\n",
        "        testLikelihood.append(distribution)\n",
        "        posterior = testLikelihood[c]* priori\n",
        "        testPosterior.append(posterior)\n",
        "        maxPosterior = testPosterior.index(max(testPosterior))\n",
        "    posteriorSpecies.append(maxPosterior)\n",
        "\n",
        "\n",
        "    testLikelihood =[]\n",
        "    testPosterior=[]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(class_test, posteriorSpecies)"
      ],
      "metadata": {
        "id": "yEXHzYftQeCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posteriorSpecies"
      ],
      "metadata": {
        "id": "ovnDDbhzdc5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VKt62jL4HID"
      },
      "source": [
        "**b. Calculate the performance metrics - Precision, Recall, Specificity and F1-score.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfkHvc024HID"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision_score(class_test, posteriorSpecies, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(class_test, posteriorSpecies, average='macro')\n"
      ],
      "metadata": {
        "id": "lKSdFXEwTLja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(class_test, posteriorSpecies, average='macro')"
      ],
      "metadata": {
        "id": "nB7HriwETgGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OUAF_YB4HID"
      },
      "source": [
        "**c. Print the Confusion Matrix.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v58ual1K4HIE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(class_test, posteriorSpecies)"
      ],
      "metadata": {
        "id": "yiDQVQ7mVPV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p27vGSt4HIE"
      },
      "source": [
        "**d. Also evaluate it using Python's Scikit-learn package.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oMjEMFP4HIE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "data = pd.read_csv(\"pima-indians-diabetes.csv\")\n",
        "Class = data[\"class\"]\n",
        "data.drop('class', inplace=True, axis=1)\n",
        "data_train,data_test,class_train,class_test = train_test_split(data, Class, test_size = 0.2, random_state=1)\n",
        "\n",
        "model = GaussianNB()\n",
        "class_pred = model.fit(data_train, class_train)\n",
        "posteriorClass = class_pred.predict(data_test)\n",
        "\n",
        "precision_score(class_test, posteriorClass)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(class_test, posteriorClass)"
      ],
      "metadata": {
        "id": "Gom8VFQGdmgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(class_test, posteriorClass,average='macro')"
      ],
      "metadata": {
        "id": "B8bywNWwYfK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(class_test, posteriorClass, average='macro')"
      ],
      "metadata": {
        "id": "edTgxb_7Yqq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(class_test, posteriorClass, average='macro')"
      ],
      "metadata": {
        "id": "k3NSQaYwY1Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(class_test, posteriorClass)"
      ],
      "metadata": {
        "id": "s_OznH56Y4wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvNlOZ26ZL2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}