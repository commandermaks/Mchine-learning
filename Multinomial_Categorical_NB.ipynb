{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/commandermaks/Mchine-learning/blob/main/Multinomial_Categorical_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f3478d",
      "metadata": {
        "id": "a7f3478d"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "The multinomial Naive Bayes classifier is suitable for classification with text data (e.g., word counts for text classification). Everything is similar to Gaussian NB except the $ P(x_{i} ∣ y) $. The new equation is,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82276efd",
      "metadata": {
        "id": "82276efd"
      },
      "source": [
        "![image_2022-08-12_171046578.png](attachment:image_2022-08-12_171046578.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27171bb1",
      "metadata": {
        "id": "27171bb1"
      },
      "source": [
        "![image_2022-08-12_173528714.png](attachment:image_2022-08-12_173528714.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623f8121",
      "metadata": {
        "id": "623f8121"
      },
      "source": [
        "## Detecting spam messages using Multinomial Naive Bayes model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502d846b",
      "metadata": {
        "id": "502d846b"
      },
      "source": [
        "**The concept of spam filtering is simple - detect spam emails from authentic (non-spam/ham) emails.\n",
        "With Bayes' Rule, we want to find the probability an email is spam, given it contains certain words. We do this by finding the probability that each word in the email is spam, and then multiply these probabilities together to get the overall email spam metric to be used in classification.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8a4fc3",
      "metadata": {
        "id": "5b8a4fc3"
      },
      "source": [
        "Probabilities can range between 0 and 1. For this spam filter, we will define that any email with a total 'spaminess' metric of over 0.5 (50%) will be deemed a spam email. When the Pr(S|W) (the probability of an email being spam S given a certain word W appears) has been found for each word in the email, they are multiplied together to give the overall probability that the email is spam. If this probability is over the 'spam threshold' of 0.5, the email is classified as a spam email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465eb93c",
      "metadata": {
        "id": "465eb93c"
      },
      "outputs": [],
      "source": [
        "# Define some training and test data for each class, spam and ham.\n",
        "\n",
        "train_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
        "train_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
        "test_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f400274",
      "metadata": {
        "id": "7f400274",
        "outputId": "5715d0d2-b4cb-40e5-acc1-748912c6a3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'send', 'your', 'password', 'send', 'us', 'your', 'account']\n"
          ]
        }
      ],
      "source": [
        "# make a vocabulary of unique words that occur in known spam emails\n",
        "\n",
        "vocab_words_spam = []\n",
        "\n",
        "for sentence in train_spam:\n",
        "    sentence_as_list = sentence.split()\n",
        "    for word in sentence_as_list:\n",
        "        vocab_words_spam.append(word)\n",
        "\n",
        "print(vocab_words_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c37f4e5",
      "metadata": {
        "id": "1c37f4e5"
      },
      "source": [
        "**Convert each list element to a dictionary key. This will delete duplicates, as dictionaries cannot have multiple keys with the same name. Convert remaining keys back to list:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76908181",
      "metadata": {
        "id": "76908181",
        "outputId": "6d64c552-b8be-464e-b990-f4554ceac1cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'account']\n"
          ]
        }
      ],
      "source": [
        "vocab_unique_words_spam = list(dict.fromkeys(vocab_words_spam))\n",
        "print(vocab_unique_words_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857c061c",
      "metadata": {
        "id": "857c061c"
      },
      "source": [
        "Spamicity' can be calculated by taking the total number of emails that have already been hand-labelled as either spam or ham, and using that data to compute word spam probabilities, by counting the frequency of each word."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251d654f",
      "metadata": {
        "id": "251d654f"
      },
      "source": [
        "We can count how many spam emails have the word “send” and divide that by the total number of spam emails - this gives a measure of the word's 'spamicity', or how likely it is to be in a spam email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b478448e",
      "metadata": {
        "id": "b478448e",
        "outputId": "6dfcc9c6-ed81-4f2c-e3af-c13defc0ae82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of spam emails with the word send: 3\n",
            "Spamicity of the word 'send': 0.6666666666666666 \n",
            "\n",
            "Number of spam emails with the word us: 2\n",
            "Spamicity of the word 'us': 0.5 \n",
            "\n",
            "Number of spam emails with the word your: 3\n",
            "Spamicity of the word 'your': 0.6666666666666666 \n",
            "\n",
            "Number of spam emails with the word password: 2\n",
            "Spamicity of the word 'password': 0.5 \n",
            "\n",
            "Number of spam emails with the word review: 1\n",
            "Spamicity of the word 'review': 0.3333333333333333 \n",
            "\n",
            "Number of spam emails with the word our: 4\n",
            "Spamicity of the word 'our': 0.8333333333333334 \n",
            "\n",
            "Number of spam emails with the word website: 1\n",
            "Spamicity of the word 'website': 0.3333333333333333 \n",
            "\n",
            "Number of spam emails with the word account: 1\n",
            "Spamicity of the word 'account': 0.3333333333333333 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "dict_spamicity = {}\n",
        "for w in vocab_unique_words_spam:\n",
        "    emails_with_w = 0     # counter\n",
        "    for sentence in train_spam:\n",
        "        if w in sentence:\n",
        "            emails_with_w+=1\n",
        "\n",
        "    print(f\"Number of spam emails with the word {w}: {emails_with_w}\")\n",
        "    total_spam = len(train_spam)\n",
        "    spamicity = (emails_with_w+1)/(total_spam+2)\n",
        "    print(f\"Spamicity of the word '{w}': {spamicity} \\n\")\n",
        "    dict_spamicity[w.lower()] = spamicity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a57ccf5",
      "metadata": {
        "id": "9a57ccf5"
      },
      "source": [
        "**Calculate Hamicity of non-spam words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2877c5f1",
      "metadata": {
        "id": "2877c5f1",
        "outputId": "3df0cd61-fd94-4343-c75c-37f5275c985f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Your', 'activity', 'report', 'benefits', 'physical', 'the', 'importance', 'vows']\n",
            "Your: Your activity report\n",
            "Number of ham emails with the word 'Your': 1\n",
            "Hamicity of the word 'Your': 0.4 \n",
            "activity: Your activity report\n",
            "activity: benefits physical activity\n",
            "Number of ham emails with the word 'activity': 2\n",
            "Hamicity of the word 'activity': 0.6 \n",
            "report: Your activity report\n",
            "Number of ham emails with the word 'report': 1\n",
            "Hamicity of the word 'report': 0.4 \n",
            "benefits: benefits physical activity\n",
            "Number of ham emails with the word 'benefits': 1\n",
            "Hamicity of the word 'benefits': 0.4 \n",
            "physical: benefits physical activity\n",
            "Number of ham emails with the word 'physical': 1\n",
            "Hamicity of the word 'physical': 0.4 \n",
            "the: the importance vows\n",
            "Number of ham emails with the word 'the': 1\n",
            "Hamicity of the word 'the': 0.4 \n",
            "importance: the importance vows\n",
            "Number of ham emails with the word 'importance': 1\n",
            "Hamicity of the word 'importance': 0.4 \n",
            "vows: the importance vows\n",
            "Number of ham emails with the word 'vows': 1\n",
            "Hamicity of the word 'vows': 0.4 \n"
          ]
        }
      ],
      "source": [
        "# make a vocabulary of unique words that occur in known ham emails\n",
        "\n",
        "vocab_words_ham = []\n",
        "\n",
        "for sentence in train_ham:\n",
        "    sentence_as_list = sentence.split()\n",
        "    for word in sentence_as_list:\n",
        "        vocab_words_ham.append(word)\n",
        "\n",
        "vocab_unique_words_ham = list(dict.fromkeys(vocab_words_ham))\n",
        "print(vocab_unique_words_ham)\n",
        "['Your', 'activity', 'report', 'benefits', 'physical', 'the', 'importance', 'vows']\n",
        "dict_hamicity = {}\n",
        "for w in vocab_unique_words_ham:\n",
        "    emails_with_w = 0     # counter\n",
        "    for sentence in train_ham:\n",
        "        if w in sentence:\n",
        "            print(w+\":\", sentence)\n",
        "            emails_with_w+=1\n",
        "\n",
        "    print(f\"Number of ham emails with the word '{w}': {emails_with_w}\")\n",
        "    total_ham = len(train_ham)\n",
        "    Hamicity = (emails_with_w+1)/(total_ham+2)       # Smoothing applied\n",
        "    print(f\"Hamicity of the word '{w}': {Hamicity} \")\n",
        "    dict_hamicity[w.lower()] = Hamicity\n",
        "# Use built-in lower() to keep all words lower case - useful later when\n",
        "# comparing spamicity vs hamicity of a single word - e.g. 'Your' and\n",
        " # 'your' will be treated as 2 different words if not normalized to lower                                          # case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33173c71",
      "metadata": {
        "id": "33173c71"
      },
      "source": [
        "**Compute Probability of Spam P(S):\n",
        "This computes the probability of any one email being spam, by dividing the total number of spam emails by the total number of all emails.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0496442b",
      "metadata": {
        "id": "0496442b",
        "outputId": "fc987229-528b-452d-c2ba-463771449d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "prob_spam = len(train_spam) / (len(train_spam)+(len(train_ham)))\n",
        "print(prob_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb718f0",
      "metadata": {
        "id": "bcb718f0"
      },
      "source": [
        "**Compute Probability of Ham P(¬S): This computes the probability of any one email being ham, by dividing the total number of ham emails by the total number of all emails.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a00fcb",
      "metadata": {
        "id": "72a00fcb",
        "outputId": "e6a7bda9-e637-4db6-d030-80dc08f14177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.42857142857142855\n"
          ]
        }
      ],
      "source": [
        "prob_ham = len(train_ham) / (len(train_spam)+(len(train_ham)))\n",
        "print(prob_ham)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ae1064",
      "metadata": {
        "id": "d3ae1064"
      },
      "source": [
        "**Given a set of un-labelled test emails, iterate over each, and create list of distinct words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0fc216e",
      "metadata": {
        "id": "f0fc216e",
        "outputId": "b3eb5493-8dc7-4a44-f841-4a0dbd82f87b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['renew your password', 'renew your vows', 'benefits of our account', 'the importance of physical activity']\n",
            "[['renew', 'your', 'password'], ['renew', 'your', 'vows'], ['benefits', 'of', 'our', 'account'], ['the', 'importance', 'of', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "tests = []\n",
        "for i in test_emails['spam']:\n",
        "    tests.append(i)\n",
        "\n",
        "for i in test_emails['ham']:\n",
        "    tests.append(i)\n",
        "\n",
        "print(tests)\n",
        "\n",
        "['renew your password', 'renew your vows', 'benefits of our account', 'the importance of physical activity']\n",
        "# split emails into distinct words\n",
        "\n",
        "distinct_words_as_sentences_test = []\n",
        "\n",
        "for sentence in tests:\n",
        "    sentence_as_list = sentence.split()\n",
        "    senten = []\n",
        "    for word in sentence_as_list:\n",
        "        senten.append(word)\n",
        "    distinct_words_as_sentences_test.append(senten)\n",
        "\n",
        "print(distinct_words_as_sentences_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77ee9ae",
      "metadata": {
        "id": "a77ee9ae",
        "outputId": "934df2e5-2488-439e-d86f-538ac95a4959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['renew', 'your', 'password'], ['renew', 'your', 'vows']]\n"
          ]
        }
      ],
      "source": [
        "test_spam_tokenized = [distinct_words_as_sentences_test[0], distinct_words_as_sentences_test[1]]\n",
        "test_ham_tokenized = [distinct_words_as_sentences_test[2], distinct_words_as_sentences_test[3]]\n",
        "print(test_spam_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae380cb9",
      "metadata": {
        "id": "ae380cb9"
      },
      "source": [
        "**Ignore the words that you haven’t seen in the labelled training data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b67370",
      "metadata": {
        "id": "56b67370",
        "outputId": "43f21ba5-c76b-47cc-e1fb-82000a7dd217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'renew', word not present in labelled spam training data\n",
            "'your', ok\n",
            "'password', ok\n",
            "'renew', word not present in labelled spam training data\n",
            "'your', ok\n",
            "'vows', ok\n",
            "[['your', 'password'], ['your', 'vows']]\n"
          ]
        }
      ],
      "source": [
        "reduced_sentences_spam_test = []\n",
        "for sentence in test_spam_tokenized:\n",
        "    words_ = []\n",
        "    for word in sentence:\n",
        "        if word in vocab_unique_words_spam:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        elif word in vocab_unique_words_ham:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        else:\n",
        "            print(f\"'{word}', word not present in labelled spam training data\")\n",
        "    reduced_sentences_spam_test.append(words_)\n",
        "print(reduced_sentences_spam_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33c7128d",
      "metadata": {
        "id": "33c7128d",
        "outputId": "d6575d57-5ecb-4e20-b8e6-25a484102b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'benefits', ok\n",
            "'of', word not present in labelled ham training data\n",
            "'our', ok\n",
            "'account', ok\n",
            "'the', ok\n",
            "'importance', ok\n",
            "'of', word not present in labelled ham training data\n",
            "'physical', ok\n",
            "'activity', ok\n",
            "[['benefits', 'our', 'account'], ['the', 'importance', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "reduced_sentences_ham_test = []                   # repeat for ham words\n",
        "for sentence in test_ham_tokenized:\n",
        "    words_ = []\n",
        "    for word in sentence:\n",
        "        if word in vocab_unique_words_ham:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        elif word in vocab_unique_words_spam:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        else:\n",
        "            print(f\"'{word}', word not present in labelled ham training data\")\n",
        "    reduced_sentences_ham_test.append(words_)\n",
        "print(reduced_sentences_ham_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6565faec",
      "metadata": {
        "id": "6565faec"
      },
      "source": [
        "**Stemming - remove non-key words: Removal of non-key words can help the classifier focus on what words are most important.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7792952d",
      "metadata": {
        "id": "7792952d",
        "outputId": "ab88e3a8-7e25-4188-b459-4bb75a67ac20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove\n",
            "remove\n",
            "[['password'], ['vows']]\n"
          ]
        }
      ],
      "source": [
        "test_spam_stemmed = []\n",
        "non_key = ['us',  'the', 'of','your']       # non-key words, gathered from spam,ham and test sentences\n",
        "for email in reduced_sentences_spam_test:\n",
        "    email_stemmed=[]\n",
        "    for word in email:\n",
        "        if word in non_key:\n",
        "            print('remove')\n",
        "        else:\n",
        "            email_stemmed.append(word)\n",
        "    test_spam_stemmed.append(email_stemmed)\n",
        "\n",
        "print(test_spam_stemmed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900ca2bb",
      "metadata": {
        "id": "900ca2bb",
        "outputId": "6d849d08-a7b9-4f18-b100-a1c44f1f8a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove\n",
            "[['benefits', 'our', 'account'], ['importance', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "test_ham_stemmed = []\n",
        "non_key = ['us',  'the', 'of', 'your']\n",
        "for email in reduced_sentences_ham_test:\n",
        "    email_stemmed=[]\n",
        "    for word in email:\n",
        "        if word in non_key:\n",
        "            print('remove')\n",
        "        else:\n",
        "            email_stemmed.append(word)\n",
        "    test_ham_stemmed.append(email_stemmed)\n",
        "\n",
        "print(test_ham_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1bf10ba",
      "metadata": {
        "id": "b1bf10ba"
      },
      "source": [
        "**Bayes' Rule**\n",
        "(To compute the probability of spam given a certain word from an email.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b1438d",
      "metadata": {
        "id": "20b1438d",
        "outputId": "eb8ae730-c050-425d-9a99-3cf2ea6c97f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "           Testing stemmed SPAM email ['password'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"password\"  is a spam word : 0.5\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for password is 0.2\n",
            "prob 'password' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'password' is spam: 0.7692307692307692\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.7692307692307692]\n",
            "email is SPAM: with spammy confidence of 76.92307692307692%\n",
            "0.7692307692307692\n",
            "\n",
            "           Testing stemmed SPAM email ['vows'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'vows' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"vows\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'vows' is spam: 0.35714285714285715\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715]\n",
            "email is HAM: with spammy confidence of 35.714285714285715%\n",
            "0.35714285714285715\n"
          ]
        }
      ],
      "source": [
        "def mult(list_) :        # function to multiply all word probs together\n",
        "    total_prob = 1\n",
        "    for i in list_:\n",
        "         total_prob = total_prob * i\n",
        "    return total_prob\n",
        "\n",
        "def Bayes(email):\n",
        "    probs = []\n",
        "    for word in email:\n",
        "        Pr_S = prob_spam\n",
        "        print('prob of spam in general ',Pr_S)\n",
        "        try:\n",
        "            pr_WS = dict_spamicity[word]\n",
        "            print(f'prob \"{word}\"  is a spam word : {pr_WS}')\n",
        "        except KeyError:\n",
        "            pr_WS = 1/(total_spam+2)  # Apply smoothing for word not seen in spam training data, but seen in ham training\n",
        "            print(f\"prob '{word}' is a spam word: {pr_WS}\")\n",
        "\n",
        "        Pr_H = prob_ham\n",
        "        print('prob of ham in general ', Pr_H)\n",
        "        try:\n",
        "            pr_WH = dict_hamicity[word]\n",
        "            print(f'prob \"{word}\" is a ham word: ',pr_WH)\n",
        "        except KeyError:\n",
        "            pr_WH = (1/(total_ham+2))  # Apply smoothing for word not seen in ham training data, but seen in spam training\n",
        "            print(f\"WH for {word} is {pr_WH}\")\n",
        "            print(f\"prob '{word}' is a ham word: {pr_WH}\")\n",
        "\n",
        "        prob_word_is_spam_BAYES = (pr_WS*Pr_S)/((pr_WS*Pr_S)+(pr_WH*Pr_H))\n",
        "        print('')\n",
        "        print(f\"Using Bayes, prob the the word '{word}' is spam: {prob_word_is_spam_BAYES}\")\n",
        "        print('###########################')\n",
        "        probs.append(prob_word_is_spam_BAYES)\n",
        "    print(f\"All word probabilities for this sentence: {probs}\")\n",
        "    final_classification = mult(probs)\n",
        "    if final_classification >= 0.5:\n",
        "        print(f'email is SPAM: with spammy confidence of {final_classification*100}%')\n",
        "    else:\n",
        "        print(f'email is HAM: with spammy confidence of {final_classification*100}%')\n",
        "    return final_classification\n",
        "for email in test_spam_stemmed:\n",
        "    print('')\n",
        "    print(f\"           Testing stemmed SPAM email {email} :\")\n",
        "    print('                 Test word by word: ')\n",
        "    all_word_probs = Bayes(email)\n",
        "    print(all_word_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d72e3e",
      "metadata": {
        "id": "c9d72e3e"
      },
      "source": [
        "**Next we test how likely the stemmed HAM test emails are to be SPAM.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b855a81",
      "metadata": {
        "id": "7b855a81",
        "outputId": "ffb72a19-7902-48bc-9349-b2212a8524c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "           Testing stemmed HAM email ['benefits', 'our', 'account'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'benefits' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"benefits\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'benefits' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"our\"  is a spam word : 0.8333333333333334\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for our is 0.2\n",
            "prob 'our' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'our' is spam: 0.847457627118644\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"account\"  is a spam word : 0.3333333333333333\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for account is 0.2\n",
            "prob 'account' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'account' is spam: 0.689655172413793\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715, 0.847457627118644, 0.689655172413793]\n",
            "email is HAM: with spammy confidence of 20.873340569424727%\n",
            "0.20873340569424728\n",
            "\n",
            "           Testing stemmed HAM email ['importance', 'physical', 'activity'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'importance' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"importance\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'importance' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'physical' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"physical\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'physical' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'activity' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"activity\" is a ham word:  0.6\n",
            "\n",
            "Using Bayes, prob the the word 'activity' is spam: 0.2702702702702703\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715, 0.35714285714285715, 0.2702702702702703]\n",
            "email is HAM: with spammy confidence of 3.447324875896305%\n",
            "0.03447324875896305\n"
          ]
        }
      ],
      "source": [
        "for email in test_ham_stemmed:\n",
        "    print('')\n",
        "    print(f\"           Testing stemmed HAM email {email} :\")\n",
        "    print('                 Test word by word: ')\n",
        "    all_word_probs = Bayes(email)\n",
        "    print(all_word_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7856d566",
      "metadata": {
        "id": "7856d566"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff60a452",
      "metadata": {
        "id": "ff60a452"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a754f1f9",
      "metadata": {
        "id": "a754f1f9"
      },
      "source": [
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
        "\n",
        "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeccb740",
      "metadata": {
        "id": "eeccb740"
      },
      "source": [
        "### Write a program to detect hate speech in tweets using the Multinomial Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd7bb82",
      "metadata": {
        "id": "8dd7bb82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8d2ac894-31be-4e0f-9a01-daca7f29abb4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2cfa8fa445e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "pWHg-CpHEqpD"
      },
      "id": "pWHg-CpHEqpD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "YbV3qcprEujL"
      },
      "id": "YbV3qcprEujL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words_racist = []\n",
        "\n",
        "racist = df_train[df_train['label']==1]['tweet']\n",
        "non_racist = df_train[df_train['label']==0]['tweet']\n",
        "\n",
        "for sentence in df_train[df_train['label']==1]['tweet']:\n",
        "  sentence_as_list = sentence.split()\n",
        "  for word in sentence_as_list:\n",
        "    vocab_words_racist.append(word)\n",
        "\n",
        "print(vocab_words_racist)\n",
        "print(len(vocab_words_racist))"
      ],
      "metadata": {
        "id": "-_fILRPnFAMZ"
      },
      "id": "-_fILRPnFAMZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_unique_words_racist = list(dict.fromkeys(vocab_words_racist))\n",
        "print(vocab_unique_words_racist)\n",
        "print(len(vocab_unique_words_racist))"
      ],
      "metadata": {
        "id": "pQH_41OKFzzp"
      },
      "id": "pQH_41OKFzzp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_racist = {}\n",
        "for w in vocab_unique_words_racist:\n",
        "    tweets_with_w = 0\n",
        "    for sentence in racist:\n",
        "        if w in sentence:\n",
        "            tweets_with_w+=1\n",
        "\n",
        "    total_racist = len(racist)\n",
        "    val = (tweets_with_w+1)/(total_racist+2)\n",
        "    dict_racist[w.lower()] = val\n",
        "\n",
        "print(total_racist)\n",
        "print(dict_racist)"
      ],
      "metadata": {
        "id": "PsGXxPNrGtOv"
      },
      "id": "PsGXxPNrGtOv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_words_nracist = []\n",
        "\n",
        "for sentence in non_racist:\n",
        "    sentence_as_list = sentence.split()\n",
        "    for word in sentence_as_list:\n",
        "        vocab_words_nracist.append(word)\n",
        "\n",
        "vocab_unique_words_nracist = list(dict.fromkeys(vocab_words_nracist))\n",
        "print(vocab_unique_words_nracist)\n",
        "print(len(vocab_unique_words_nracist))\n",
        "\n",
        "total_nracist = len(non_racist)\n",
        "print(total_nracist)\n",
        "\n",
        "\n",
        "dict_nracist = {}\n",
        "for w in vocab_unique_words_nracist:\n",
        "    emails_with_w = 0     # counter\n",
        "    for sentence in non_racist:\n",
        "        if w in sentence:\n",
        "            emails_with_w+=1\n",
        "\n",
        "    val = (emails_with_w+1)/(total_nracist+2)\n",
        "    dict_nracist[w.lower()] = val\n",
        "\n",
        "print(dict_nracist)"
      ],
      "metadata": {
        "id": "KY-MnnPbIgTc"
      },
      "id": "KY-MnnPbIgTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_racist = len(racist) / (len(racist)+(len(non_racist)))\n",
        "print(prob_racist)"
      ],
      "metadata": {
        "id": "3Nv02PauOtVJ"
      },
      "id": "3Nv02PauOtVJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_nracist = len(non_racist) / (len(racist)+(len(non_racist)))\n",
        "print(prob_nracist)"
      ],
      "metadata": {
        "id": "OoEY1_rcPBBE"
      },
      "id": "OoEY1_rcPBBE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/test.csv')\n",
        "tests = test['tweet']\n",
        "\n",
        "distinct_words_as_sentences_test = []\n",
        "\n",
        "for sentence in tests:\n",
        "    sentence_as_list = sentence.split()\n",
        "    senten = []\n",
        "    for word in sentence_as_list:\n",
        "        senten.append(word)\n",
        "    distinct_words_as_sentences_test.append(senten)\n",
        "\n",
        "print(distinct_words_as_sentences_test)"
      ],
      "metadata": {
        "id": "Pf-aLUDGPEAt"
      },
      "id": "Pf-aLUDGPEAt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_sentences_test = []\n",
        "for sentence in distinct_words_as_sentences_test:\n",
        "    words_ = []\n",
        "    for word in sentence:\n",
        "        if word in vocab_unique_words_racist:\n",
        "            words_.append(word)\n",
        "        elif word in vocab_unique_words_nracist:\n",
        "           . words_append(word)\n",
        "    reduced_sentences_test.append(words_)\n",
        "print(reduced_sentences_test)"
      ],
      "metadata": {
        "id": "tiOvpSoVPuHa"
      },
      "id": "tiOvpSoVPuHa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "non_key = stopwords.words('english')\n",
        "\n",
        "test_stemmed = []\n",
        "\n",
        "for tweet in reduced_sentences_test:\n",
        "    tweet_stemmed=[]\n",
        "    for word in tweet:\n",
        "        if word in non_key:\n",
        "            print(f'{word} removed')\n",
        "        else:\n",
        "            tweet_stemmed.append(word)\n",
        "    test_stemmed.append(tweet_stemmed)\n",
        "\n",
        "print(test_stemmed)"
      ],
      "metadata": {
        "id": "SKRSUQkFQZl5"
      },
      "id": "SKRSUQkFQZl5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mult(list_) :\n",
        "    total_prob = 1\n",
        "    for i in list_:\n",
        "         total_prob = total_prob * i\n",
        "    return total_prob\n",
        "\n",
        "def Bayes(tweet):\n",
        "    probs = []\n",
        "    for word in tweet:\n",
        "        Pr_R = prob_racist\n",
        "        try:\n",
        "            pr_WR = dict_racist[word]\n",
        "        except KeyError:\n",
        "            pr_WR = 1/(total_racist+2)\n",
        "\n",
        "        Pr_N = prob_nracist\n",
        "        try:\n",
        "            pr_WN = dict_nracist[word]\n",
        "        except KeyError:\n",
        "            pr_WN = (1/(total_nracist+2))\n",
        "\n",
        "        prob_word_is_racist_BAYES = (pr_WR*Pr_R)/((pr_WR*Pr_R)+(pr_WN*Pr_N))\n",
        "        probs.append(prob_word_is_racist_BAYES)\n",
        "\n",
        "    final_classification = mult(probs)\n",
        "    if final_classification >= 0.5:\n",
        "        return \"Racist tweet\"\n",
        "    else:\n",
        "        return \"Non-Racist tweet\""
      ],
      "metadata": {
        "id": "Y9FDpBRvTKv9"
      },
      "id": "Y9FDpBRvTKv9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = []\n",
        "\n",
        "r=0\n",
        "for tweet in test_stemmed:\n",
        "  s = Bayes(tweet)\n",
        "  if(s==\"Racist tweet\"):\n",
        "    r+=1\n",
        "  final.append(s)\n",
        "\n",
        "print(final)\n",
        "print(f'Racist tweets : {r}')\n",
        "print(f'Non-Racist tweets : {len(test_stemmed)-r}')"
      ],
      "metadata": {
        "id": "nFmdz_W7U59q"
      },
      "id": "nFmdz_W7U59q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "906c2d38",
      "metadata": {
        "id": "906c2d38"
      },
      "source": [
        "## Categorical Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1c9e61",
      "metadata": {
        "id": "3d1c9e61"
      },
      "source": [
        "It is suitable for classification with discrete features which assumes categorically distribution for each feature. The features should to encoded using label encoding  techniques such that each category would be mapped to a unique number."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1494adc",
      "metadata": {
        "id": "d1494adc"
      },
      "source": [
        "The probability of category $t$ in feature $i$ given class $c$ is estimated as:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3804513",
      "metadata": {
        "id": "e3804513"
      },
      "source": [
        "![categorical.PNG](attachment:categorical.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a900ff44",
      "metadata": {
        "id": "a900ff44"
      },
      "source": [
        "![parameter_categorical.PNG](attachment:parameter_categorical.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e195573",
      "metadata": {
        "id": "7e195573"
      },
      "source": [
        "## Step By Step Implementation of Categorical Naive Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2369752d",
      "metadata": {
        "id": "2369752d"
      },
      "source": [
        "1. Preprocessing the data.\n",
        "2. Calculate the counts/presence of each feature based on class.\n",
        "3. Calculate likelihood probability.\n",
        "4. Calculate prior probability.\n",
        "5. Calculate posterior probability for a given query point → Predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325dd403",
      "metadata": {
        "id": "325dd403"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OrdinalEncoder,LabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad42274",
      "metadata": {
        "id": "cad42274"
      },
      "outputs": [],
      "source": [
        "weather = ['Clear', 'Clear', 'Clear', 'Clear', 'Clear', 'Clear',\n",
        "            'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy',\n",
        "            'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy']\n",
        "\n",
        "timeOfWeek = ['Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend',\n",
        "            'Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend',\n",
        "            'Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend']\n",
        "\n",
        "timeOfDay = ['Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            ]\n",
        "trafficJam = ['Yes', 'No', 'Yes',\n",
        "            'No', 'No', 'No',\n",
        "            'Yes', 'Yes', 'Yes',\n",
        "            'No', 'No', 'No',\n",
        "            'Yes', 'Yes', 'Yes',\n",
        "            'Yes', 'No', 'Yes'\n",
        "            ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05809e84",
      "metadata": {
        "id": "05809e84",
        "outputId": "2053d863-ecab-4c4e-d215-5006ad2fdc8f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>weather</th>\n",
              "      <th>timeOfWeek</th>\n",
              "      <th>timeOfDay</th>\n",
              "      <th>trafficJam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Morning</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Morning</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Clear</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Evening</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Morning</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Morning</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Rainy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Evening</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Morning</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Workday</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Morning</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Lunch</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Snowy</td>\n",
              "      <td>Weekend</td>\n",
              "      <td>Evening</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   weather timeOfWeek timeOfDay trafficJam\n",
              "0    Clear    Workday   Morning        Yes\n",
              "1    Clear    Workday     Lunch         No\n",
              "2    Clear    Workday   Evening        Yes\n",
              "3    Clear    Weekend   Morning         No\n",
              "4    Clear    Weekend     Lunch         No\n",
              "5    Clear    Weekend   Evening         No\n",
              "6    Rainy    Workday   Morning        Yes\n",
              "7    Rainy    Workday     Lunch        Yes\n",
              "8    Rainy    Workday   Evening        Yes\n",
              "9    Rainy    Weekend   Morning         No\n",
              "10   Rainy    Weekend     Lunch         No\n",
              "11   Rainy    Weekend   Evening         No\n",
              "12   Snowy    Workday   Morning        Yes\n",
              "13   Snowy    Workday     Lunch        Yes\n",
              "14   Snowy    Workday   Evening        Yes\n",
              "15   Snowy    Weekend   Morning        Yes\n",
              "16   Snowy    Weekend     Lunch         No\n",
              "17   Snowy    Weekend   Evening        Yes"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(zip(weather,timeOfWeek,timeOfDay,trafficJam),columns = ['weather','timeOfWeek','timeOfDay','trafficJam'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e2a48b",
      "metadata": {
        "id": "e5e2a48b"
      },
      "outputs": [],
      "source": [
        "weather = df['weather'].values.reshape(-1,1)\n",
        "timeOfWeek = df['timeOfWeek'].values.reshape(-1,1)\n",
        "timeOfDay = df['timeOfDay'].values.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2369505f",
      "metadata": {
        "id": "2369505f",
        "outputId": "c6d62d1a-2a7c-44ed-8851-9c16b374e64c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((18, 1), (18, 1))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weather.shape,timeOfWeek.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec62a27a",
      "metadata": {
        "id": "ec62a27a"
      },
      "outputs": [],
      "source": [
        "def preprocess():\n",
        "    # Using ordinal encoder to convert the categories in the range from 0 to n-1\n",
        "    wea_enc = OrdinalEncoder()\n",
        "    weather_ = wea_enc.fit_transform(weather)\n",
        "\n",
        "    timeOfWeek_enc = OrdinalEncoder()\n",
        "    timeOfWeek_ = timeOfWeek_enc.fit_transform(timeOfWeek)\n",
        "\n",
        "    timeOfDay_enc = OrdinalEncoder()\n",
        "    timeOfDay_ = timeOfDay_enc.fit_transform(timeOfDay)\n",
        "    # Stacking all the features\n",
        "    X = np.column_stack((weather_,timeOfWeek_,timeOfDay_))\n",
        "    # Changing the type to int\n",
        "    X = X.astype(int)\n",
        "    # Doing one hot encoding on the target data\n",
        "    y = df['trafficJam']\n",
        "    lb = LabelBinarizer()\n",
        "    y_ = lb.fit_transform(y)\n",
        "    if y_.shape[1] == 1:\n",
        "        y_ = np.concatenate((1 - y_, y_), axis=1)\n",
        "    return X,y_,lb.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5900500c",
      "metadata": {
        "id": "5900500c"
      },
      "source": [
        "**Preprocessing the data:\n",
        "Converting the categorical data into a numerical form using ordinal encoding. The features are converted to ordinal integers.\n",
        "This results in a single column of integers (0 to n_categories — 1) per feature.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136e95d2",
      "metadata": {
        "id": "136e95d2",
        "outputId": "e92a0145-8b88-47aa-d1d5-14b096ccb5e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((18, 3), (18, 2))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X,y,classes = preprocess()\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964165ab",
      "metadata": {
        "id": "964165ab"
      },
      "outputs": [],
      "source": [
        "def counts_based_onclass(X,y):\n",
        "\n",
        "    # No of feature\n",
        "    n_features = X.shape[1]\n",
        "    # No of classes\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    count_matrix = []\n",
        "    # For each feature\n",
        "    for i in range(n_features):\n",
        "        count_feature = []\n",
        "        # Get that particuar feature from the dataset\n",
        "        X_feature = X[:,i]\n",
        "        # For each class\n",
        "        for j in range(n_classes):\n",
        "            # Get the datapoints that belong to the class - j\n",
        "            mask = y[:,j].astype(bool)\n",
        "            # Using masking filter out the datapoints that belong to this class- j in the given feature - i\n",
        "            # Using bincount -- count all the different categories present in the given feature\n",
        "            counts = np.bincount(X_feature[mask])\n",
        "\n",
        "            count_feature.append(counts)\n",
        "\n",
        "        count_matrix.append(np.array(count_feature))\n",
        "        # Finding the count of datapoints beloging to each class -- we will use it to calculate prior probabilities.\n",
        "        class_count = y.sum(axis=0)\n",
        "\n",
        "    return count_matrix,n_features,n_classes,class_count\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e7ff948",
      "metadata": {
        "id": "6e7ff948"
      },
      "outputs": [],
      "source": [
        "count_matrix,n_features,n_classes,class_count = counts_based_onclass(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a68862",
      "metadata": {
        "id": "79a68862",
        "outputId": "1f76e45a-1204-4b75-8dcf-35cc2772e748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[4, 3, 1],\n",
              "        [2, 3, 5]], dtype=int64),\n",
              " array([[7, 1],\n",
              "        [2, 8]], dtype=int64),\n",
              " array([[2, 4, 2],\n",
              "        [4, 2, 4]], dtype=int64)]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count_matrix will give an output this way, For each of the features you have 2D -array\n",
        "#(The first row corresponding to No and the second row corresponding to Yes)\n",
        "\n",
        "count_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855b14de",
      "metadata": {
        "id": "855b14de"
      },
      "outputs": [],
      "source": [
        "def calculate_likelihood_probs(count_matrix,alpha,n_features):\n",
        "    log_probabilities = []\n",
        "    for i in range(n_features):\n",
        "        num = count_matrix[i] + alpha\n",
        "        den = num.sum(axis = 1).reshape(-1,1)\n",
        "        log_probability = np.log(num) - np.log(den)\n",
        "        log_probabilities.append(log_probability)\n",
        "    return log_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8970f888",
      "metadata": {
        "id": "8970f888"
      },
      "outputs": [],
      "source": [
        "def calculate_prior_probs(class_count):\n",
        "\n",
        "    num = class_count\n",
        "    den = class_count.sum()\n",
        "\n",
        "    return np.log(num)-np.log(den)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c605ffda",
      "metadata": {
        "id": "c605ffda"
      },
      "outputs": [],
      "source": [
        "prior_probs = calculate_prior_probs(class_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab27af8f",
      "metadata": {
        "id": "ab27af8f"
      },
      "outputs": [],
      "source": [
        "log_probs = calculate_likelihood_probs(count_matrix,1,n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0187b8",
      "metadata": {
        "id": "6e0187b8"
      },
      "outputs": [],
      "source": [
        "def predict(query_point,log_probs,prior_probs):\n",
        "\n",
        "    # Intializing an empty array\n",
        "    probs = np.zeros((1,n_classes))\n",
        "    # For each feature\n",
        "    for i in range(n_features):\n",
        "        # Get the category_id of the feature - i from the query_point\n",
        "        category = query_point[i]\n",
        "        # Fetch the corresponding log_probability table and add continue to add them for all the features\n",
        "        probs+=log_probs[i][:,category]\n",
        "    # Finally add posterior probability\n",
        "    probs+=prior_probs\n",
        "    # Finding the maximum of the probabilities and fetching the corresponding class\n",
        "    return classes[np.argmax(probs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4538d31",
      "metadata": {
        "id": "f4538d31",
        "outputId": "c079e3f6-45dc-4e3d-8946-98d1efc056c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Likelihood probabilities\n",
            " [array([[-0.78845736, -1.01160091, -1.70474809],\n",
            "       [-1.46633707, -1.178655  , -0.77318989]]), array([[-0.22314355, -1.60943791],\n",
            "       [-1.38629436, -0.28768207]]), array([[-1.29928298, -0.78845736, -1.29928298],\n",
            "       [-0.95551145, -1.46633707, -0.95551145]])]\n",
            "Prior probabilities\n",
            " [-0.81093022 -0.58778666]\n"
          ]
        }
      ],
      "source": [
        "print('Likelihood probabilities\\n',log_probs)\n",
        "print('Prior probabilities\\n',prior_probs)\n",
        "#print('Predict',predict(X[4],log_probs,prior_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2cd151",
      "metadata": {
        "id": "ec2cd151"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2671a7aa",
      "metadata": {
        "id": "2671a7aa"
      },
      "source": [
        "**Dataset characteristics:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72b42b1",
      "metadata": {
        "id": "c72b42b1"
      },
      "source": [
        "1. Number of instances: 1000\n",
        "2. Number of attributes: 5 (including target attribute), all categorical\n",
        "3. Attribute information:\n",
        "    * size (XS, S, M, L, XL, XXL, 3XL)\n",
        "    * material (nylon, polyester, silk, cotton, linen)\n",
        "    * color (white, cream, blue, black, orange, green, yellow, red, violet, navy)\n",
        "    * sleeves (short, long)\n",
        "    * demand (low, medium, high)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db507fdf",
      "metadata": {
        "id": "db507fdf"
      },
      "source": [
        "### Write a program to implement the Categorical Naive Bayes classification algorithm to predict clothing demand (low, medium, high) based on the rest of the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b950e99f",
      "metadata": {
        "id": "b950e99f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OrdinalEncoder,LabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Clothing.csv', index_col=0)"
      ],
      "metadata": {
        "id": "ee5PpJLnWe_2"
      },
      "id": "ee5PpJLnWe_2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ECZVZgejXLij",
        "outputId": "b7d6f8a0-83df-4e2d-8262-e16d8520a8f8"
      },
      "id": "ECZVZgejXLij",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  size   material   color sleeves  demand\n",
              "0    S      nylon   white    long  medium\n",
              "1   XL  polyester   cream   short    high\n",
              "2    S       silk    blue   short  medium\n",
              "3    M     cotton   black   short  medium\n",
              "4   XL  polyester  orange    long  medium"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e9e17f4-6230-4b81-b2fe-88c82039d732\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>size</th>\n",
              "      <th>material</th>\n",
              "      <th>color</th>\n",
              "      <th>sleeves</th>\n",
              "      <th>demand</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S</td>\n",
              "      <td>nylon</td>\n",
              "      <td>white</td>\n",
              "      <td>long</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XL</td>\n",
              "      <td>polyester</td>\n",
              "      <td>cream</td>\n",
              "      <td>short</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S</td>\n",
              "      <td>silk</td>\n",
              "      <td>blue</td>\n",
              "      <td>short</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>cotton</td>\n",
              "      <td>black</td>\n",
              "      <td>short</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XL</td>\n",
              "      <td>polyester</td>\n",
              "      <td>orange</td>\n",
              "      <td>long</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e9e17f4-6230-4b81-b2fe-88c82039d732')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e9e17f4-6230-4b81-b2fe-88c82039d732 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e9e17f4-6230-4b81-b2fe-88c82039d732');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = df['size'].values.reshape(-1,1)\n",
        "material = df['material'].values.reshape(-1,1)\n",
        "color = df['color'].values.reshape(-1,1)\n",
        "sleeves = df['sleeves'].values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Sj_Yo3InXNJG"
      },
      "id": "Sj_Yo3InXNJG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size.shape, material.shape, color.shape, sleeves.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjufhGmXYGa_",
        "outputId": "0cc356b9-d026-407b-a8c0-9f92c634d9e9"
      },
      "id": "TjufhGmXYGa_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 1), (10000, 1), (10000, 1), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess():\n",
        "    size_enc = OrdinalEncoder()\n",
        "    size_ = size_enc.fit_transform(size)\n",
        "\n",
        "    material_enc = OrdinalEncoder()\n",
        "    material_ = material_enc.fit_transform(material)\n",
        "\n",
        "    color_enc = OrdinalEncoder()\n",
        "    color_ = color_enc.fit_transform(color)\n",
        "\n",
        "    sleeves_enc = OrdinalEncoder()\n",
        "    sleeves_ = sleeves_enc.fit_transform(sleeves)\n",
        "\n",
        "    X = np.column_stack((size_,material_,color_,sleeves_))\n",
        "    X = X.astype(int)\n",
        "    y = df['demand']\n",
        "    lb = LabelBinarizer()\n",
        "    y_ = lb.fit_transform(y)\n",
        "    if y_.shape[1] == 1:\n",
        "        y_ = np.concatenate((1 - y_, y_), axis=1)\n",
        "    return X,y_,lb.classes_"
      ],
      "metadata": {
        "id": "uBrUq0xHYNJh"
      },
      "id": "uBrUq0xHYNJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,classes = preprocess()\n",
        "X.shape, y.shape, classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUjRFXS0Zz8u",
        "outputId": "019b88ec-b5eb-485e-9ecb-4725200055dc"
      },
      "id": "vUjRFXS0Zz8u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 4), (10000, 3), array(['high', 'low', 'medium'], dtype='<U6'))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjr8Q9XSnDE5"
      },
      "id": "fjr8Q9XSnDE5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def counts_based_onclass(X,y):\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    count_matrix = []\n",
        "    for i in range(n_features):\n",
        "        count_feature = []\n",
        "        X_feature = X[:,i]\n",
        "        bin = len(np.bincount(X_feature))\n",
        "        for j in range(n_classes):\n",
        "            mask = y[:,j].astype(bool)\n",
        "            counts = np.bincount(X_feature[mask])\n",
        "            a=len(counts)\n",
        "\n",
        "            for k in range(bin-a):\n",
        "              counts = np.append(counts,[0])\n",
        "\n",
        "            print(counts)\n",
        "\n",
        "            count_feature.append(counts)\n",
        "\n",
        "        count_matrix.append(np.array(count_feature))\n",
        "        class_count = y.sum(axis=0)\n",
        "\n",
        "    return count_matrix,n_features,n_classes,class_count"
      ],
      "metadata": {
        "id": "ceOX5O1xZ3az"
      },
      "id": "ceOX5O1xZ3az",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_matrix,n_features,n_classes,class_count = counts_based_onclass(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c73d9xnbZ-BA",
        "outputId": "749fe1b5-87ac-459d-b128-5dc7e6049aaa"
      },
      "id": "c73d9xnbZ-BA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 142 1169  707  835  622  509  345]\n",
            "[344 110 154  28 277  45 345]\n",
            "[359 952 866 421 841 329 600]\n",
            "[ 866 1090 1185 1188    0]\n",
            "[214  64 266 287 472]\n",
            "[1074  479 1349 1365  101]\n",
            "[  0 597 687 196 660 338 588 315 614 334]\n",
            "[111  86  57  84  73 112  97 117 441 125]\n",
            "[1427  317  379  261  399  209  313  230  591  242]\n",
            "[1034 3295]\n",
            "[864 439]\n",
            "[3146 1222]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Z7qA2t2ldZ1",
        "outputId": "02ad71cf-40d9-46cf-8b6a-78bc47d2707a"
      },
      "id": "4Z7qA2t2ldZ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 142, 1169,  707,  835,  622,  509,  345],\n",
              "        [ 344,  110,  154,   28,  277,   45,  345],\n",
              "        [ 359,  952,  866,  421,  841,  329,  600]]),\n",
              " array([[ 866, 1090, 1185, 1188,    0],\n",
              "        [ 214,   64,  266,  287,  472],\n",
              "        [1074,  479, 1349, 1365,  101]]),\n",
              " array([[   0,  597,  687,  196,  660,  338,  588,  315,  614,  334],\n",
              "        [ 111,   86,   57,   84,   73,  112,   97,  117,  441,  125],\n",
              "        [1427,  317,  379,  261,  399,  209,  313,  230,  591,  242]]),\n",
              " array([[1034, 3295],\n",
              "        [ 864,  439],\n",
              "        [3146, 1222]])]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_likelihood_probs(count_matrix,alpha,n_features):\n",
        "    log_probabilities = []\n",
        "    for i in range(n_features):\n",
        "        num = count_matrix[i] + alpha\n",
        "        print(num)\n",
        "        den = num.sum(axis=1).reshape(-1,1)\n",
        "        print(den)\n",
        "        log_probability = np.log(num) - np.log(den)\n",
        "        log_probabilities.append(log_probability)\n",
        "    return log_probabilities"
      ],
      "metadata": {
        "id": "ysAfAaVOa3Qn"
      },
      "id": "ysAfAaVOa3Qn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_prior_probs(class_count):\n",
        "\n",
        "    num = class_count\n",
        "    den = class_count.sum()\n",
        "\n",
        "    return np.log(num)-np.log(den)"
      ],
      "metadata": {
        "id": "eaE9B5eSbVeQ"
      },
      "id": "eaE9B5eSbVeQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior_probs = calculate_prior_probs(class_count)"
      ],
      "metadata": {
        "id": "St-4Ic6CbbgS"
      },
      "id": "St-4Ic6CbbgS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = calculate_likelihood_probs(count_matrix,1,n_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIYjqjcjbejH",
        "outputId": "8e258e11-39e8-4a6d-81d5-1b1ba8ddc17c"
      },
      "id": "BIYjqjcjbejH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 143 1170  708  836  623  510  346]\n",
            " [ 345  111  155   29  278   46  346]\n",
            " [ 360  953  867  422  842  330  601]]\n",
            "[[4336]\n",
            " [1310]\n",
            " [4375]]\n",
            "[[ 867 1091 1186 1189    1]\n",
            " [ 215   65  267  288  473]\n",
            " [1075  480 1350 1366  102]]\n",
            "[[4334]\n",
            " [1308]\n",
            " [4373]]\n",
            "[[   1  598  688  197  661  339  589  316  615  335]\n",
            " [ 112   87   58   85   74  113   98  118  442  126]\n",
            " [1428  318  380  262  400  210  314  231  592  243]]\n",
            "[[4339]\n",
            " [1313]\n",
            " [4378]]\n",
            "[[1035 3296]\n",
            " [ 865  440]\n",
            " [3147 1223]]\n",
            "[[4331]\n",
            " [1305]\n",
            " [4370]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(query_point,log_probs,prior_probs):\n",
        "\n",
        "    # Intializing an empty array\n",
        "    probs = np.zeros((1,n_classes))\n",
        "    # For each feature\n",
        "    for i in range(n_features):\n",
        "        # Get the category_id of the feature - i from the query_point\n",
        "        category = query_point[i]\n",
        "        # Fetch the corresponding log_probability table and add continue to add them for all the features\n",
        "        probs+=log_probs[i][:,category]\n",
        "    # Finally add posterior probability\n",
        "    probs+=prior_probs\n",
        "    # Finding the maximum of the probabilities and fetching the corresponding class\n",
        "    return classes[np.argmax(probs)]"
      ],
      "metadata": {
        "id": "dyev-9KYbhif"
      },
      "id": "dyev-9KYbhif",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Likelihood probabilities\\n',log_probs)\n",
        "print('Prior probabilities\\n',prior_probs)\n",
        "print('Predict',predict(X[4],log_probs,prior_probs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyTWBKfwsl9o",
        "outputId": "91fea7f5-6af4-4141-ffa8-7dc5c947c0c3"
      },
      "id": "jyTWBKfwsl9o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Likelihood probabilities\n",
            " [array([[-3.41186291, -1.30994852, -1.81226345, -1.64607893, -1.94016102,\n",
            "        -2.14029682, -2.52826877],\n",
            "       [-1.334238  , -2.46825221, -2.1343573 , -3.81048659, -1.5501613 ,\n",
            "        -3.34914102, -1.33134364],\n",
            "       [-2.49755777, -1.5240469 , -1.61862282, -2.33865648, -1.64788178,\n",
            "        -2.58456914, -1.98506686]]), array([[-1.60920721, -1.3793962 , -1.2959046 , -1.29337829, -8.37424618],\n",
            "       [-1.8056165 , -3.00186726, -1.58900587, -1.51329405, -1.01715914],\n",
            "       [-1.40312861, -2.20941845, -1.17534468, -1.16356251, -3.75823174]]), array([[-8.37539919, -1.98180843, -1.84161035, -3.09219546, -1.88164535,\n",
            "        -2.54939908, -1.996973  , -2.61965697, -1.95377692, -2.56126865],\n",
            "       [-2.461571  , -2.71416176, -3.11962686, -2.73741862, -2.87600478,\n",
            "        -2.45268206, -2.5951024 , -2.40938525, -1.08875999, -2.34378797],\n",
            "       [-1.12031714, -2.6222959 , -2.44417603, -2.81600277, -2.39288273,\n",
            "        -3.03723975, -2.63495429, -2.94192957, -2.00084064, -2.89128583]]), array([[-1.43139704, -0.27308885],\n",
            "       [-0.41122881, -1.08718359],\n",
            "       [-0.32831339, -1.27345615]])]\n",
            "Prior probabilities\n",
            " [-0.83724852 -2.03791579 -0.82827985]\n",
            "Predict medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCYmjt-Rsp6S",
        "outputId": "e680dc42-d61c-4a77-fe43-9fc8e865bbbe"
      },
      "id": "JCYmjt-Rsp6S",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWNVYlv-s4I5"
      },
      "id": "kWNVYlv-s4I5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7856d566",
        "7e195573",
        "ec2cd151"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}