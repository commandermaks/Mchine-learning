{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/commandermaks/Mchine-learning/blob/main/Multinomial_Categorical_NB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f3478d",
      "metadata": {
        "id": "a7f3478d"
      },
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "The multinomial Naive Bayes classifier is suitable for classification with text data (e.g., word counts for text classification). Everything is similar to Gaussian NB except the $ P(x_{i} ∣ y) $. The new equation is,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82276efd",
      "metadata": {
        "id": "82276efd"
      },
      "source": [
        "![image_2022-08-12_171046578.png](attachment:image_2022-08-12_171046578.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27171bb1",
      "metadata": {
        "id": "27171bb1"
      },
      "source": [
        "![image_2022-08-12_173528714.png](attachment:image_2022-08-12_173528714.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623f8121",
      "metadata": {
        "id": "623f8121"
      },
      "source": [
        "## Detecting spam messages using Multinomial Naive Bayes model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502d846b",
      "metadata": {
        "id": "502d846b"
      },
      "source": [
        "**The concept of spam filtering is simple - detect spam emails from authentic (non-spam/ham) emails.\n",
        "With Bayes' Rule, we want to find the probability an email is spam, given it contains certain words. We do this by finding the probability that each word in the email is spam, and then multiply these probabilities together to get the overall email spam metric to be used in classification.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8a4fc3",
      "metadata": {
        "id": "5b8a4fc3"
      },
      "source": [
        "Probabilities can range between 0 and 1. For this spam filter, we will define that any email with a total 'spaminess' metric of over 0.5 (50%) will be deemed a spam email. When the Pr(S|W) (the probability of an email being spam S given a certain word W appears) has been found for each word in the email, they are multiplied together to give the overall probability that the email is spam. If this probability is over the 'spam threshold' of 0.5, the email is classified as a spam email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "465eb93c",
      "metadata": {
        "id": "465eb93c"
      },
      "outputs": [],
      "source": [
        "# Define some training and test data for each class, spam and ham.\n",
        "\n",
        "train_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
        "train_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
        "test_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f400274",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f400274",
        "outputId": "3a51ce6f-442d-4ccb-d1fd-4a263233f80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'send', 'your', 'password', 'send', 'us', 'your', 'account']\n"
          ]
        }
      ],
      "source": [
        "# make a vocabulary of unique words that occur in known spam emails\n",
        "\n",
        "vocab_words_spam = []\n",
        "\n",
        "for sentence in train_spam:\n",
        "    sentence_as_list = sentence.split()\n",
        "    for word in sentence_as_list:\n",
        "        vocab_words_spam.append(word)\n",
        "\n",
        "print(vocab_words_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c37f4e5",
      "metadata": {
        "id": "1c37f4e5"
      },
      "source": [
        "**Convert each list element to a dictionary key. This will delete duplicates, as dictionaries cannot have multiple keys with the same name. Convert remaining keys back to list:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76908181",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76908181",
        "outputId": "6e59ba67-cf67-403b-b166-fa579609ba39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['send', 'us', 'your', 'password', 'review', 'our', 'website', 'account']\n"
          ]
        }
      ],
      "source": [
        "vocab_unique_words_spam = list(dict.fromkeys(vocab_words_spam))\n",
        "print(vocab_unique_words_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "857c061c",
      "metadata": {
        "id": "857c061c"
      },
      "source": [
        "Spamicity' can be calculated by taking the total number of emails that have already been hand-labelled as either spam or ham, and using that data to compute word spam probabilities, by counting the frequency of each word."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251d654f",
      "metadata": {
        "id": "251d654f"
      },
      "source": [
        "We can count how many spam emails have the word “send” and divide that by the total number of spam emails - this gives a measure of the word's 'spamicity', or how likely it is to be in a spam email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b478448e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b478448e",
        "outputId": "e44bc164-4100-4605-e0c0-f5713754b359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of spam emails with the word send: 3\n",
            "Spamicity of the word 'send': 0.6666666666666666 \n",
            "\n",
            "Number of spam emails with the word us: 2\n",
            "Spamicity of the word 'us': 0.5 \n",
            "\n",
            "Number of spam emails with the word your: 3\n",
            "Spamicity of the word 'your': 0.6666666666666666 \n",
            "\n",
            "Number of spam emails with the word password: 2\n",
            "Spamicity of the word 'password': 0.5 \n",
            "\n",
            "Number of spam emails with the word review: 1\n",
            "Spamicity of the word 'review': 0.3333333333333333 \n",
            "\n",
            "Number of spam emails with the word our: 4\n",
            "Spamicity of the word 'our': 0.8333333333333334 \n",
            "\n",
            "Number of spam emails with the word website: 1\n",
            "Spamicity of the word 'website': 0.3333333333333333 \n",
            "\n",
            "Number of spam emails with the word account: 1\n",
            "Spamicity of the word 'account': 0.3333333333333333 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "dict_spamicity = {}\n",
        "for w in vocab_unique_words_spam:\n",
        "    emails_with_w = 0     # counter\n",
        "    for sentence in train_spam:\n",
        "        if w in sentence:\n",
        "            emails_with_w+=1\n",
        "\n",
        "    print(f\"Number of spam emails with the word {w}: {emails_with_w}\")\n",
        "    total_spam = len(train_spam)\n",
        "    spamicity = (emails_with_w+1)/(total_spam+2)\n",
        "    print(f\"Spamicity of the word '{w}': {spamicity} \\n\")\n",
        "    dict_spamicity[w.lower()] = spamicity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a57ccf5",
      "metadata": {
        "id": "9a57ccf5"
      },
      "source": [
        "**Calculate Hamicity of non-spam words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2877c5f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2877c5f1",
        "outputId": "06054aba-aca0-40db-8feb-4c3f3f3d4013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Your', 'activity', 'report', 'benefits', 'physical', 'the', 'importance', 'vows']\n",
            "Your: Your activity report\n",
            "Number of ham emails with the word 'Your': 1\n",
            "Hamicity of the word 'Your': 0.4 \n",
            "activity: Your activity report\n",
            "activity: benefits physical activity\n",
            "Number of ham emails with the word 'activity': 2\n",
            "Hamicity of the word 'activity': 0.6 \n",
            "report: Your activity report\n",
            "Number of ham emails with the word 'report': 1\n",
            "Hamicity of the word 'report': 0.4 \n",
            "benefits: benefits physical activity\n",
            "Number of ham emails with the word 'benefits': 1\n",
            "Hamicity of the word 'benefits': 0.4 \n",
            "physical: benefits physical activity\n",
            "Number of ham emails with the word 'physical': 1\n",
            "Hamicity of the word 'physical': 0.4 \n",
            "the: the importance vows\n",
            "Number of ham emails with the word 'the': 1\n",
            "Hamicity of the word 'the': 0.4 \n",
            "importance: the importance vows\n",
            "Number of ham emails with the word 'importance': 1\n",
            "Hamicity of the word 'importance': 0.4 \n",
            "vows: the importance vows\n",
            "Number of ham emails with the word 'vows': 1\n",
            "Hamicity of the word 'vows': 0.4 \n"
          ]
        }
      ],
      "source": [
        "# make a vocabulary of unique words that occur in known ham emails\n",
        "\n",
        "vocab_words_ham = []\n",
        "\n",
        "for sentence in train_ham:\n",
        "    sentence_as_list = sentence.split()\n",
        "    for word in sentence_as_list:\n",
        "        vocab_words_ham.append(word)\n",
        "\n",
        "vocab_unique_words_ham = list(dict.fromkeys(vocab_words_ham))\n",
        "print(vocab_unique_words_ham)\n",
        "['Your', 'activity', 'report', 'benefits', 'physical', 'the', 'importance', 'vows']\n",
        "dict_hamicity = {}\n",
        "for w in vocab_unique_words_ham:\n",
        "    emails_with_w = 0     # counter\n",
        "    for sentence in train_ham:\n",
        "        if w in sentence:\n",
        "            print(w+\":\", sentence)\n",
        "            emails_with_w+=1\n",
        "\n",
        "    print(f\"Number of ham emails with the word '{w}': {emails_with_w}\")\n",
        "    total_ham = len(train_ham)\n",
        "    Hamicity = (emails_with_w+1)/(total_ham+2)       # Smoothing applied\n",
        "    print(f\"Hamicity of the word '{w}': {Hamicity} \")\n",
        "    dict_hamicity[w.lower()] = Hamicity\n",
        "# Use built-in lower() to keep all words lower case - useful later when\n",
        "# comparing spamicity vs hamicity of a single word - e.g. 'Your' and\n",
        " # 'your' will be treated as 2 different words if not normalized to lower                                          # case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33173c71",
      "metadata": {
        "id": "33173c71"
      },
      "source": [
        "**Compute Probability of Spam P(S):\n",
        "This computes the probability of any one email being spam, by dividing the total number of spam emails by the total number of all emails.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0496442b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0496442b",
        "outputId": "0e627ceb-b2c5-4ca6-9d13-e43c67855cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "prob_spam = len(train_spam) / (len(train_spam)+(len(train_ham)))\n",
        "print(prob_spam)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb718f0",
      "metadata": {
        "id": "bcb718f0"
      },
      "source": [
        "**Compute Probability of Ham P(¬S): This computes the probability of any one email being ham, by dividing the total number of ham emails by the total number of all emails.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "72a00fcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72a00fcb",
        "outputId": "8b37037d-7262-4358-e8b0-25ae2bbddc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42857142857142855\n"
          ]
        }
      ],
      "source": [
        "prob_ham = len(train_ham) / (len(train_spam)+(len(train_ham)))\n",
        "print(prob_ham)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ae1064",
      "metadata": {
        "id": "d3ae1064"
      },
      "source": [
        "**Given a set of un-labelled test emails, iterate over each, and create list of distinct words:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f0fc216e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0fc216e",
        "outputId": "54070954-6f6c-4028-851c-76dd9283c068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['renew your password', 'renew your vows', 'benefits of our account', 'the importance of physical activity']\n",
            "[['renew', 'your', 'password'], ['renew', 'your', 'vows'], ['benefits', 'of', 'our', 'account'], ['the', 'importance', 'of', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "tests = []\n",
        "for i in test_emails['spam']:\n",
        "    tests.append(i)\n",
        "\n",
        "for i in test_emails['ham']:\n",
        "    tests.append(i)\n",
        "\n",
        "print(tests)\n",
        "\n",
        "['renew your password', 'renew your vows', 'benefits of our account', 'the importance of physical activity']\n",
        "# split emails into distinct words\n",
        "\n",
        "distinct_words_as_sentences_test = []\n",
        "\n",
        "for sentence in tests:\n",
        "    sentence_as_list = sentence.split()\n",
        "    senten = []\n",
        "    for word in sentence_as_list:\n",
        "        senten.append(word)\n",
        "    distinct_words_as_sentences_test.append(senten)\n",
        "\n",
        "print(distinct_words_as_sentences_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a77ee9ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a77ee9ae",
        "outputId": "f7cec895-a7a1-40d2-b3ab-e63b0b5f7b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['renew', 'your', 'password'], ['renew', 'your', 'vows']]\n"
          ]
        }
      ],
      "source": [
        "test_spam_tokenized = [distinct_words_as_sentences_test[0], distinct_words_as_sentences_test[1]]\n",
        "test_ham_tokenized = [distinct_words_as_sentences_test[2], distinct_words_as_sentences_test[3]]\n",
        "print(test_spam_tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae380cb9",
      "metadata": {
        "id": "ae380cb9"
      },
      "source": [
        "**Ignore the words that you haven’t seen in the labelled training data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "56b67370",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56b67370",
        "outputId": "91a61e11-307a-44b5-95e8-501dcca6c514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'renew', word not present in labelled spam training data\n",
            "'your', ok\n",
            "'password', ok\n",
            "'renew', word not present in labelled spam training data\n",
            "'your', ok\n",
            "'vows', ok\n",
            "[['your', 'password'], ['your', 'vows']]\n"
          ]
        }
      ],
      "source": [
        "reduced_sentences_spam_test = []\n",
        "for sentence in test_spam_tokenized:\n",
        "    words_ = []\n",
        "    for word in sentence:\n",
        "        if word in vocab_unique_words_spam:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        elif word in vocab_unique_words_ham:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        else:\n",
        "            print(f\"'{word}', word not present in labelled spam training data\")\n",
        "    reduced_sentences_spam_test.append(words_)\n",
        "print(reduced_sentences_spam_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "33c7128d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33c7128d",
        "outputId": "3e994c75-05bc-4b79-9a62-d45ecf7eeeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'benefits', ok\n",
            "'of', word not present in labelled ham training data\n",
            "'our', ok\n",
            "'account', ok\n",
            "'the', ok\n",
            "'importance', ok\n",
            "'of', word not present in labelled ham training data\n",
            "'physical', ok\n",
            "'activity', ok\n",
            "[['benefits', 'our', 'account'], ['the', 'importance', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "reduced_sentences_ham_test = []                   # repeat for ham words\n",
        "for sentence in test_ham_tokenized:\n",
        "    words_ = []\n",
        "    for word in sentence:\n",
        "        if word in vocab_unique_words_ham:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        elif word in vocab_unique_words_spam:\n",
        "            print(f\"'{word}', ok\")\n",
        "            words_.append(word)\n",
        "        else:\n",
        "            print(f\"'{word}', word not present in labelled ham training data\")\n",
        "    reduced_sentences_ham_test.append(words_)\n",
        "print(reduced_sentences_ham_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6565faec",
      "metadata": {
        "id": "6565faec"
      },
      "source": [
        "**Stemming - remove non-key words: Removal of non-key words can help the classifier focus on what words are most important.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7792952d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7792952d",
        "outputId": "e3458162-52da-4d97-8b3b-366976c6fa0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove\n",
            "remove\n",
            "[['password'], ['vows']]\n"
          ]
        }
      ],
      "source": [
        "test_spam_stemmed = []\n",
        "non_key = ['us',  'the', 'of','your']       # non-key words, gathered from spam,ham and test sentences\n",
        "for email in reduced_sentences_spam_test:\n",
        "    email_stemmed=[]\n",
        "    for word in email:\n",
        "        if word in non_key:\n",
        "            print('remove')\n",
        "        else:\n",
        "            email_stemmed.append(word)\n",
        "    test_spam_stemmed.append(email_stemmed)\n",
        "\n",
        "print(test_spam_stemmed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "900ca2bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900ca2bb",
        "outputId": "f1e1e5db-9e87-4b74-80cb-161e778f69af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove\n",
            "[['benefits', 'our', 'account'], ['importance', 'physical', 'activity']]\n"
          ]
        }
      ],
      "source": [
        "test_ham_stemmed = []\n",
        "non_key = ['us',  'the', 'of', 'your']\n",
        "for email in reduced_sentences_ham_test:\n",
        "    email_stemmed=[]\n",
        "    for word in email:\n",
        "        if word in non_key:\n",
        "            print('remove')\n",
        "        else:\n",
        "            email_stemmed.append(word)\n",
        "    test_ham_stemmed.append(email_stemmed)\n",
        "\n",
        "print(test_ham_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1bf10ba",
      "metadata": {
        "id": "b1bf10ba"
      },
      "source": [
        "**Bayes' Rule**\n",
        "(To compute the probability of spam given a certain word from an email.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "20b1438d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20b1438d",
        "outputId": "f5613ef7-e5b3-4ceb-ca31-10c93a0fc69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "           Testing stemmed SPAM email ['password'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"password\"  is a spam word : 0.5\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for password is 0.2\n",
            "prob 'password' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'password' is spam: 0.7692307692307692\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.7692307692307692]\n",
            "email is SPAM: with spammy confidence of 76.92307692307692%\n",
            "0.7692307692307692\n",
            "\n",
            "           Testing stemmed SPAM email ['vows'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'vows' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"vows\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'vows' is spam: 0.35714285714285715\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715]\n",
            "email is HAM: with spammy confidence of 35.714285714285715%\n",
            "0.35714285714285715\n"
          ]
        }
      ],
      "source": [
        "def mult(list_) :        # function to multiply all word probs together\n",
        "    total_prob = 1\n",
        "    for i in list_:\n",
        "         total_prob = total_prob * i\n",
        "    return total_prob\n",
        "\n",
        "def Bayes(email):\n",
        "    probs = []\n",
        "    for word in email:\n",
        "        Pr_S = prob_spam\n",
        "        print('prob of spam in general ',Pr_S)\n",
        "        try:\n",
        "            pr_WS = dict_spamicity[word]\n",
        "            print(f'prob \"{word}\"  is a spam word : {pr_WS}')\n",
        "        except KeyError:\n",
        "            pr_WS = 1/(total_spam+2)  # Apply smoothing for word not seen in spam training data, but seen in ham training\n",
        "            print(f\"prob '{word}' is a spam word: {pr_WS}\")\n",
        "\n",
        "        Pr_H = prob_ham\n",
        "        print('prob of ham in general ', Pr_H)\n",
        "        try:\n",
        "            pr_WH = dict_hamicity[word]\n",
        "            print(f'prob \"{word}\" is a ham word: ',pr_WH)\n",
        "        except KeyError:\n",
        "            pr_WH = (1/(total_ham+2))  # Apply smoothing for word not seen in ham training data, but seen in spam training\n",
        "            print(f\"WH for {word} is {pr_WH}\")\n",
        "            print(f\"prob '{word}' is a ham word: {pr_WH}\")\n",
        "\n",
        "        prob_word_is_spam_BAYES = (pr_WS*Pr_S)/((pr_WS*Pr_S)+(pr_WH*Pr_H))\n",
        "        print('')\n",
        "        print(f\"Using Bayes, prob the the word '{word}' is spam: {prob_word_is_spam_BAYES}\")\n",
        "        print('###########################')\n",
        "        probs.append(prob_word_is_spam_BAYES)\n",
        "    print(f\"All word probabilities for this sentence: {probs}\")\n",
        "    final_classification = mult(probs)\n",
        "    if final_classification >= 0.5:\n",
        "        print(f'email is SPAM: with spammy confidence of {final_classification*100}%')\n",
        "    else:\n",
        "        print(f'email is HAM: with spammy confidence of {final_classification*100}%')\n",
        "    return final_classification\n",
        "for email in test_spam_stemmed:\n",
        "    print('')\n",
        "    print(f\"           Testing stemmed SPAM email {email} :\")\n",
        "    print('                 Test word by word: ')\n",
        "    all_word_probs = Bayes(email)\n",
        "    print(all_word_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d72e3e",
      "metadata": {
        "id": "c9d72e3e"
      },
      "source": [
        "**Next we test how likely the stemmed HAM test emails are to be SPAM.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7b855a81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b855a81",
        "outputId": "1405ff85-79f7-470a-9270-6557a6b6e80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "           Testing stemmed HAM email ['benefits', 'our', 'account'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'benefits' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"benefits\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'benefits' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"our\"  is a spam word : 0.8333333333333334\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for our is 0.2\n",
            "prob 'our' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'our' is spam: 0.847457627118644\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob \"account\"  is a spam word : 0.3333333333333333\n",
            "prob of ham in general  0.42857142857142855\n",
            "WH for account is 0.2\n",
            "prob 'account' is a ham word: 0.2\n",
            "\n",
            "Using Bayes, prob the the word 'account' is spam: 0.689655172413793\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715, 0.847457627118644, 0.689655172413793]\n",
            "email is HAM: with spammy confidence of 20.873340569424727%\n",
            "0.20873340569424728\n",
            "\n",
            "           Testing stemmed HAM email ['importance', 'physical', 'activity'] :\n",
            "                 Test word by word: \n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'importance' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"importance\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'importance' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'physical' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"physical\" is a ham word:  0.4\n",
            "\n",
            "Using Bayes, prob the the word 'physical' is spam: 0.35714285714285715\n",
            "###########################\n",
            "prob of spam in general  0.5714285714285714\n",
            "prob 'activity' is a spam word: 0.16666666666666666\n",
            "prob of ham in general  0.42857142857142855\n",
            "prob \"activity\" is a ham word:  0.6\n",
            "\n",
            "Using Bayes, prob the the word 'activity' is spam: 0.2702702702702703\n",
            "###########################\n",
            "All word probabilities for this sentence: [0.35714285714285715, 0.35714285714285715, 0.2702702702702703]\n",
            "email is HAM: with spammy confidence of 3.447324875896305%\n",
            "0.03447324875896305\n"
          ]
        }
      ],
      "source": [
        "for email in test_ham_stemmed:\n",
        "    print('')\n",
        "    print(f\"           Testing stemmed HAM email {email} :\")\n",
        "    print('                 Test word by word: ')\n",
        "    all_word_probs = Bayes(email)\n",
        "    print(all_word_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7856d566",
      "metadata": {
        "id": "7856d566"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff60a452",
      "metadata": {
        "id": "ff60a452"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a754f1f9",
      "metadata": {
        "id": "a754f1f9"
      },
      "source": [
        "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
        "\n",
        "Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeccb740",
      "metadata": {
        "id": "eeccb740"
      },
      "source": [
        "### Write a program to detect hate speech in tweets using the Multinomial Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8dd7bb82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "8dd7bb82",
        "outputId": "7e57da3c-faf6-4f92-acff-da6bc870d40c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ef7812bafbc8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data =pd.read_csv('test.csv')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "906c2d38",
      "metadata": {
        "id": "906c2d38"
      },
      "source": [
        "## Categorical Naïve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d1c9e61",
      "metadata": {
        "id": "3d1c9e61"
      },
      "source": [
        "It is suitable for classification with discrete features which assumes categorically distribution for each feature. The features should to encoded using label encoding  techniques such that each category would be mapped to a unique number."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1494adc",
      "metadata": {
        "id": "d1494adc"
      },
      "source": [
        "The probability of category $t$ in feature $i$ given class $c$ is estimated as:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3804513",
      "metadata": {
        "id": "e3804513"
      },
      "source": [
        "![categorical.PNG](attachment:categorical.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a900ff44",
      "metadata": {
        "id": "a900ff44"
      },
      "source": [
        "![parameter_categorical.PNG](attachment:parameter_categorical.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e195573",
      "metadata": {
        "id": "7e195573"
      },
      "source": [
        "## Step By Step Implementation of Categorical Naive Bayes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2369752d",
      "metadata": {
        "id": "2369752d"
      },
      "source": [
        "1. Preprocessing the data.\n",
        "2. Calculate the counts/presence of each feature based on class.\n",
        "3. Calculate likelihood probability.\n",
        "4. Calculate prior probability.\n",
        "5. Calculate posterior probability for a given query point → Predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325dd403",
      "metadata": {
        "id": "325dd403"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OrdinalEncoder,LabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad42274",
      "metadata": {
        "id": "cad42274"
      },
      "outputs": [],
      "source": [
        "weather = ['Clear', 'Clear', 'Clear', 'Clear', 'Clear', 'Clear',\n",
        "            'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy', 'Rainy',\n",
        "            'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy', 'Snowy']\n",
        "\n",
        "timeOfWeek = ['Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend',\n",
        "            'Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend',\n",
        "            'Workday', 'Workday', 'Workday',\n",
        "            'Weekend', 'Weekend', 'Weekend']\n",
        "\n",
        "timeOfDay = ['Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            'Morning', 'Lunch', 'Evening',\n",
        "            ]\n",
        "trafficJam = ['Yes', 'No', 'Yes',\n",
        "            'No', 'No', 'No',\n",
        "            'Yes', 'Yes', 'Yes',\n",
        "            'No', 'No', 'No',\n",
        "            'Yes', 'Yes', 'Yes',\n",
        "            'Yes', 'No', 'Yes'\n",
        "            ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05809e84",
      "metadata": {
        "id": "05809e84"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(zip(weather,timeOfWeek,timeOfDay,trafficJam),columns = ['weather','timeOfWeek','timeOfDay','trafficJam'])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e2a48b",
      "metadata": {
        "id": "e5e2a48b"
      },
      "outputs": [],
      "source": [
        "weather = df['weather'].values.reshape(-1,1)\n",
        "timeOfWeek = df['timeOfWeek'].values.reshape(-1,1)\n",
        "timeOfDay = df['timeOfDay'].values.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2369505f",
      "metadata": {
        "id": "2369505f"
      },
      "outputs": [],
      "source": [
        "weather.shape,timeOfWeek.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec62a27a",
      "metadata": {
        "id": "ec62a27a"
      },
      "outputs": [],
      "source": [
        "def preprocess():\n",
        "    # Using ordinal encoder to convert the categories in the range from 0 to n-1\n",
        "    wea_enc = OrdinalEncoder()\n",
        "    weather_ = wea_enc.fit_transform(weather)\n",
        "\n",
        "    timeOfWeek_enc = OrdinalEncoder()\n",
        "    timeOfWeek_ = timeOfWeek_enc.fit_transform(timeOfWeek)\n",
        "\n",
        "    timeOfDay_enc = OrdinalEncoder()\n",
        "    timeOfDay_ = timeOfDay_enc.fit_transform(timeOfDay)\n",
        "    # Stacking all the features\n",
        "    X = np.column_stack((weather_,timeOfWeek_,timeOfDay_))\n",
        "    # Changing the type to int\n",
        "    X = X.astype(int)\n",
        "    # Doing one hot encoding on the target data\n",
        "    y = df['trafficJam']\n",
        "    lb = LabelBinarizer()\n",
        "    y_ = lb.fit_transform(y)\n",
        "    if y_.shape[1] == 1:\n",
        "        y_ = np.concatenate((1 - y_, y_), axis=1)\n",
        "    return X,y_,lb.classes_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5900500c",
      "metadata": {
        "id": "5900500c"
      },
      "source": [
        "**Preprocessing the data:\n",
        "Converting the categorical data into a numerical form using ordinal encoding. The features are converted to ordinal integers.\n",
        "This results in a single column of integers (0 to n_categories — 1) per feature.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136e95d2",
      "metadata": {
        "id": "136e95d2"
      },
      "outputs": [],
      "source": [
        "X,y,classes = preprocess()\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964165ab",
      "metadata": {
        "id": "964165ab"
      },
      "outputs": [],
      "source": [
        "def counts_based_onclass(X,y):\n",
        "\n",
        "    # No of feature\n",
        "    n_features = X.shape[1]\n",
        "    # No of classes\n",
        "    n_classes = y.shape[1]\n",
        "\n",
        "    count_matrix = []\n",
        "    # For each feature\n",
        "    for i in range(n_features):\n",
        "        count_feature = []\n",
        "        # Get that particuar feature from the dataset\n",
        "        X_feature = X[:,i]\n",
        "        # For each class\n",
        "        for j in range(n_classes):\n",
        "            # Get the datapoints that belong to the class - j\n",
        "            mask = y[:,j].astype(bool)\n",
        "            # Using masking filter out the datapoints that belong to this class- j in the given feature - i\n",
        "            # Using bincount -- count all the different categories present in the given feature\n",
        "            counts = np.bincount(X_feature[mask])\n",
        "\n",
        "            count_feature.append(counts)\n",
        "\n",
        "        count_matrix.append(np.array(count_feature))\n",
        "        # Finding the count of datapoints beloging to each class -- we will use it to calculate prior probabilities.\n",
        "        class_count = y.sum(axis=0)\n",
        "\n",
        "    return count_matrix,n_features,n_classes,class_count\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e7ff948",
      "metadata": {
        "id": "6e7ff948"
      },
      "outputs": [],
      "source": [
        "count_matrix,n_features,n_classes,class_count = counts_based_onclass(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a68862",
      "metadata": {
        "id": "79a68862"
      },
      "outputs": [],
      "source": [
        "# Count_matrix will give an output this way, For each of the features you have 2D -array\n",
        "#(The first row corresponding to No and the second row corresponding to Yes)\n",
        "\n",
        "count_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855b14de",
      "metadata": {
        "id": "855b14de"
      },
      "outputs": [],
      "source": [
        "def calculate_likelihood_probs(count_matrix,alpha,n_features):\n",
        "    log_probabilities = []\n",
        "    for i in range(n_features):\n",
        "        num = count_matrix[i] + alpha\n",
        "        den = num.sum(axis = 1).reshape(-1,1)\n",
        "        log_probability = np.log(num) - np.log(den)\n",
        "        log_probabilities.append(log_probability)\n",
        "    return log_probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8970f888",
      "metadata": {
        "id": "8970f888"
      },
      "outputs": [],
      "source": [
        "def calculate_prior_probs(class_count):\n",
        "\n",
        "    num = class_count\n",
        "    den = class_count.sum()\n",
        "\n",
        "    return np.log(num)-np.log(den)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c605ffda",
      "metadata": {
        "id": "c605ffda"
      },
      "outputs": [],
      "source": [
        "prior_probs = calculate_prior_probs(class_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab27af8f",
      "metadata": {
        "id": "ab27af8f"
      },
      "outputs": [],
      "source": [
        "log_probs = calculate_likelihood_probs(count_matrix,1,n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0187b8",
      "metadata": {
        "id": "6e0187b8"
      },
      "outputs": [],
      "source": [
        "def predict(query_point,log_probs,prior_probs):\n",
        "\n",
        "    # Intializing an empty array\n",
        "    probs = np.zeros((1,n_classes))\n",
        "    # For each feature\n",
        "    for i in range(n_features):\n",
        "        # Get the category_id of the feature - i from the query_point\n",
        "        category = query_point[i]\n",
        "        # Fetch the corresponding log_probability table and add continue to add them for all the features\n",
        "        probs+=log_probs[i][:,category]\n",
        "    # Finally add posterior probability\n",
        "    probs+=prior_probs\n",
        "    # Finding the maximum of the probabilities and fetching the corresponding class\n",
        "    return classes[np.argmax(probs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4538d31",
      "metadata": {
        "id": "f4538d31"
      },
      "outputs": [],
      "source": [
        "print('Likelihood probabilities\\n',log_probs)\n",
        "print('Prior probabilities\\n',prior_probs)\n",
        "#print('Predict',predict(X[4],log_probs,prior_probs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec2cd151",
      "metadata": {
        "id": "ec2cd151"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2671a7aa",
      "metadata": {
        "id": "2671a7aa"
      },
      "source": [
        "**Dataset characteristics:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72b42b1",
      "metadata": {
        "id": "c72b42b1"
      },
      "source": [
        "1. Number of instances: 1000\n",
        "2. Number of attributes: 5 (including target attribute), all categorical\n",
        "3. Attribute information:\n",
        "    * size (XS, S, M, L, XL, XXL, 3XL)\n",
        "    * material (nylon, polyester, silk, cotton, linen)\n",
        "    * color (white, cream, blue, black, orange, green, yellow, red, violet, navy)\n",
        "    * sleeves (short, long)\n",
        "    * demand (low, medium, high)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db507fdf",
      "metadata": {
        "id": "db507fdf"
      },
      "source": [
        "### Write a program to implement the Categorical Naive Bayes classification algorithm to predict clothing demand (low, medium, high) based on the rest of the attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b950e99f",
      "metadata": {
        "id": "b950e99f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}